{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95804254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_t2m_c import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27202a7",
   "metadata": {},
   "source": [
    "Inital code\n",
    "encode-> add motion in between -> decode\n",
    "encode the prev motion(bvh) -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11769574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batch_size: 32\n",
      "checkpoints_dir: ./log/t2m\n",
      "cond_scale: 4\n",
      "dataset_name: t2m\n",
      "dropout: 0.2\n",
      "ext: text2motion\n",
      "ff_size: 1024\n",
      "force_mask: False\n",
      "gpu_id: -1\n",
      "gumbel_sample: False\n",
      "is_train: False\n",
      "latent_dim: 384\n",
      "mask_edit_section: None\n",
      "max_motion_length: 196\n",
      "motion_length: 0\n",
      "n_heads: 6\n",
      "n_layers: 8\n",
      "name: t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns\n",
      "num_batch: 2\n",
      "repeat_times: 1\n",
      "res_name: tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\n",
      "seed: 1\n",
      "source_motion: example_data/000612.npy\n",
      "temperature: 1.0\n",
      "text_path: \n",
      "text_prompt: \n",
      "time_steps: 10\n",
      "topkr: 0.9\n",
      "unit_length: 4\n",
      "use_res_model: False\n",
      "vq_name: rvq_nq1_dc512_nc512\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['']  # Prevent argparse from picking up unwanted Jupyter arguments\n",
    "\n",
    "# from gen_t2m import EvalT2MOptions  # or from wherever your file is\n",
    "\n",
    "# Initialize and parse\n",
    "parser = EvalT2MOptions()\n",
    "opt = parser.parse(is_eval=True)\n",
    "\n",
    "# Set values as per your CLI\n",
    "opt.res_name = \"tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\"\n",
    "opt.name = \"2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\"\n",
    "opt.text_prompt = \"the person crouches and walks forward.\"\n",
    "opt.motion_length = -1\n",
    "opt.repeat_times = 3\n",
    "opt.gpu_id = -1\n",
    "opt.seed = 1\n",
    "opt.ext = \"generation_name_nopredlen\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8da79c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixseed(opt.seed)\n",
    "\n",
    "opt.device = torch.device(\"cpu\" if opt.gpu_id == -1 else \"cuda:\" + str(opt.gpu_id))\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "dim_pose = 251 if opt.dataset_name == 'kit' else 263\n",
    "\n",
    "# out_dir = pjoin(opt.check)\n",
    "root_dir = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)\n",
    "model_dir = pjoin(root_dir, 'model')\n",
    "result_dir = pjoin('./generation', opt.ext)\n",
    "joints_dir = pjoin(result_dir, 'joints')\n",
    "animation_dir = pjoin(result_dir, 'animations')\n",
    "os.makedirs(joints_dir, exist_ok=True)\n",
    "os.makedirs(animation_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e1a3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./log/t2m/t2m/2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd/opt.txt\n"
     ]
    }
   ],
   "source": [
    "model_opt_path = pjoin(root_dir, 'opt.txt')\n",
    "model_opt = get_opt(model_opt_path, device=opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "76d0cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./log/vq/t2m/rvq_nq6_dc512_nc512_noshare_qdp0.2/opt.txt\n",
      "Loading VQ Model rvq_nq6_dc512_nc512_noshare_qdp0.2 Completed!\n"
     ]
    }
   ],
   "source": [
    "global vq_opt # This now works as vq_opt is global\n",
    "# generator = BVHGenerator()\n",
    "\n",
    "vq_opt_path = pjoin('./log/vq', opt.dataset_name, model_opt.vq_name, 'opt.txt')\n",
    "vq_opt = get_opt(vq_opt_path, device=opt.device)\n",
    "vq_model , vq_opt = load_vq_model(vq_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84649d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.num_tokens = vq_opt.nb_code\n",
    "model_opt.num_quantizers = vq_opt.num_quantizers\n",
    "model_opt.code_dim = vq_opt.code_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23e854f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_res_model(res_opt,vq_opt):\n",
    "    res_opt.num_quantizers = vq_opt.num_quantizers\n",
    "    res_opt.num_tokens = vq_opt.nb_code\n",
    "    res_transformer = ResidualTransformer(code_dim=vq_opt.code_dim,\n",
    "                                            cond_mode='text',\n",
    "                                            latent_dim=res_opt.latent_dim,\n",
    "                                            ff_size=res_opt.ff_size,\n",
    "                                            num_layers=res_opt.n_layers,\n",
    "                                            num_heads=res_opt.n_heads,\n",
    "                                            dropout=res_opt.dropout,\n",
    "                                            clip_dim=512,\n",
    "                                            shared_codebook=vq_opt.shared_codebook,\n",
    "                                            cond_drop_prob=res_opt.cond_drop_prob,\n",
    "                                            # codebook=vq_model.quantizer.codebooks[0] if opt.fix_token_emb else None,\n",
    "                                            share_weight=res_opt.share_weight,\n",
    "                                            clip_version=clip_version,\n",
    "                                            opt=res_opt)\n",
    "\n",
    "    ckpt = torch.load(pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                      map_location=opt.device)\n",
    "    missing_keys, unexpected_keys = res_transformer.load_state_dict(ckpt['res_transformer'], strict=False)\n",
    "    assert len(unexpected_keys) == 0\n",
    "    assert all([k.startswith('clip_model.') for k in missing_keys])\n",
    "    print(f'Loading Residual Transformer {res_opt.name} from epoch {ckpt[\"ep\"]}!')\n",
    "    return res_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9d4cd34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw/opt.txt\n",
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "Loading Residual Transformer tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw from epoch 440!\n"
     ]
    }
   ],
   "source": [
    "res_opt_path = pjoin('checkpoints', opt.dataset_name, opt.res_name, 'opt.txt')\n",
    "res_opt = get_opt(res_opt_path, device=opt.device)\n",
    "res_model = load_res_model(res_opt,vq_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca670b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623dea69",
   "metadata": {},
   "source": [
    "Joints to features code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6afb97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from common.skeleton import Skeleton\n",
    "from utils.paramUtil import *\n",
    "from common.quaternion import qrot_np, qinv_np, qmul_np, quaternion_to_cont6d_np, qbetween_np, qfix\n",
    "\n",
    "def joints_to_features(joints, feet_thre=0.002, dataset='humanml'):\n",
    "    \"\"\"\n",
    "    joints: numpy array of shape (frames, joints_num, 3)\n",
    "    returns: RIC feature vector (frames-1, feature_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == 'humanml':\n",
    "        n_raw_offsets = torch.from_numpy(t2m_raw_offsets)\n",
    "        kinematic_chain = t2m_kinematic_chain\n",
    "        joints_num = 22\n",
    "        fid_r, fid_l = [8, 11], [7, 10]\n",
    "        face_joint_indx = [2, 1, 17, 16]\n",
    "    elif dataset == 'kit':\n",
    "        n_raw_offsets = torch.from_numpy(kit_raw_offsets)\n",
    "        kinematic_chain = kit_kinematic_chain\n",
    "        joints_num = 21\n",
    "        fid_r, fid_l = [14, 15], [19, 20]\n",
    "        face_joint_indx = [11, 16, 5, 8]\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset')\n",
    "\n",
    "    joints = torch.from_numpy(joints).float()\n",
    "\n",
    "    # Step 1: Build Skeleton\n",
    "    skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "\n",
    "    # Step 2: Uniform skeleton (optional, skipped if already processed)\n",
    "    tgt_offsets = skel.get_offsets_joints(joints[0])\n",
    "\n",
    "    # Step 3: Put on floor\n",
    "    floor_height = joints[:, :, 1].min()\n",
    "    joints[:, :, 1] -= floor_height\n",
    "\n",
    "    # Step 4: Face Z+\n",
    "    r_hip, l_hip, sdr_r, sdr_l = face_joint_indx\n",
    "    across1 = joints[0, r_hip] - joints[0, l_hip]\n",
    "    across2 = joints[0, sdr_r] - joints[0, sdr_l]\n",
    "    across = across1 + across2\n",
    "    across = across / torch.norm(across)\n",
    "    forward_init = torch.cross(torch.tensor([0.0, 1.0, 0.0]), across)\n",
    "    forward_init = forward_init / torch.norm(forward_init)\n",
    "    target = torch.tensor([0.0, 0.0, 1.0])\n",
    "    root_quat_init = qbetween_np(forward_init.numpy(), target.numpy())\n",
    "    root_quat_init = np.ones(joints.shape[:-1] + (4,)) * root_quat_init\n",
    "    joints = qrot_np(root_quat_init, joints.numpy())\n",
    "\n",
    "    global_positions = joints.copy()\n",
    "\n",
    "    # Step 5: Foot contact detection\n",
    "    def foot_detect(positions, thres):\n",
    "        velfactor = np.array([thres, thres])\n",
    "        feet_l_x = (positions[1:, fid_l, 0] - positions[:-1, fid_l, 0]) ** 2\n",
    "        feet_l_y = (positions[1:, fid_l, 1] - positions[:-1, fid_l, 1]) ** 2\n",
    "        feet_l_z = (positions[1:, fid_l, 2] - positions[:-1, fid_l, 2]) ** 2\n",
    "        feet_l = ((feet_l_x + feet_l_y + feet_l_z) < velfactor).astype(np.float32)\n",
    "\n",
    "        feet_r_x = (positions[1:, fid_r, 0] - positions[:-1, fid_r, 0]) ** 2\n",
    "        feet_r_y = (positions[1:, fid_r, 1] - positions[:-1, fid_r, 1]) ** 2\n",
    "        feet_r_z = (positions[1:, fid_r, 2] - positions[:-1, fid_r, 2]) ** 2\n",
    "        feet_r = ((feet_r_x + feet_r_y + feet_r_z) < velfactor).astype(np.float32)\n",
    "\n",
    "        return feet_l, feet_r\n",
    "\n",
    "    feet_l, feet_r = foot_detect(joints, feet_thre)\n",
    "\n",
    "    # Step 6: Extract features\n",
    "\n",
    "    # Inverse kinematics to quaternions\n",
    "    skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "    quat_params = skel.inverse_kinematics_np(joints, face_joint_indx, smooth_forward=True)\n",
    "    quat_params = qfix(quat_params)\n",
    "\n",
    "    # Continuous 6D\n",
    "    cont_6d_params = quaternion_to_cont6d_np(quat_params)\n",
    "    r_rot = quat_params[:, 0]\n",
    "\n",
    "    # Root linear velocities\n",
    "    velocity = joints[1:, 0] - joints[:-1, 0]\n",
    "    velocity = qrot_np(r_rot[1:], velocity)\n",
    "    l_velocity = velocity[:, [0, 2]]\n",
    "\n",
    "    # Root rotation velocities\n",
    "    r_velocity = qmul_np(r_rot[1:], qinv_np(r_rot[:-1]))\n",
    "    r_velocity = np.arcsin(r_velocity[:, 2:3])\n",
    "\n",
    "    # Root height\n",
    "    root_y = joints[:-1, 0, 1:2]\n",
    "\n",
    "    root_data = np.concatenate([r_velocity, l_velocity, root_y], axis=-1)\n",
    "\n",
    "    # Joint positions (RIC)\n",
    "    joints_local = joints.copy()\n",
    "    joints_local[..., 0] -= joints_local[:, 0:1, 0]\n",
    "    joints_local[..., 2] -= joints_local[:, 0:1, 2]\n",
    "    joints_local = qrot_np(np.repeat(r_rot[:, None], joints_local.shape[1], axis=1), joints_local)\n",
    "    ric_data = joints_local[:, 1:].reshape(joints_local.shape[0], -1)\n",
    "\n",
    "    # Joint rotations\n",
    "    rot_data = cont_6d_params[:, 1:].reshape(cont_6d_params.shape[0], -1)\n",
    "\n",
    "    # Joint velocities\n",
    "    local_vel = qrot_np(np.repeat(r_rot[:-1, None], global_positions.shape[1], axis=1),\n",
    "                        global_positions[1:] - global_positions[:-1])\n",
    "    local_vel = local_vel.reshape(local_vel.shape[0], -1)\n",
    "\n",
    "    # Combine\n",
    "    data = np.concatenate([root_data, ric_data[:-1], rot_data[:-1], local_vel, feet_l, feet_r], axis=-1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0db2a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(r\"generation/generation_name_nopredlen/joints/0/sample0_repeat2_len196_ik.npy\", allow_pickle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34dfff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIC Features Shape: (195, 263)\n"
     ]
    }
   ],
   "source": [
    "ric_features = joints_to_features(test, feet_thre=0.002, dataset='humanml')\n",
    "print(\"RIC Features Shape:\", ric_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "72d20e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'mean.npy'))\n",
    "std = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'std.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5229a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_joints = (ric_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c774cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ric_tensor = torch.from_numpy(normed_joints).float().unsqueeze(0).to('cpu')  # (1, seq_len, 263)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59bc0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized, tokens = vq_model.encode(ric_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2fc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f02ce9",
   "metadata": {},
   "source": [
    "Inbetween code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a02df2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 20\n",
    "t_sec=10 # addtion at this second\n",
    "downsample_factor=5\n",
    "frame_index = int(t_sec * FPS)\n",
    "insert_frame = int(frame_index / downsample_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6fbf8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"the person runs in forward direction\",\n",
    "     \n",
    "]\n",
    "est_length = True\n",
    "length_list = []\n",
    "\n",
    "# if est_length:\n",
    "#     print(\"Since no motion length are specified, we will use estimated motion lengthes!!\")\n",
    "#     text_embedding = t2m_transformer.encode_text(prompt_list)\n",
    "#     pred_dis = length_estimator(text_embedding)\n",
    "#     probs = F.softmax(pred_dis, dim=-1)  # (b, ntoken)\n",
    "#     token_lens = Categorical(probs).sample()  # (b, seqlen)\n",
    "#     # lengths = torch.multinomial()\n",
    "# else:\n",
    "token_lens = torch.LongTensor(length_list) // 4\n",
    "token_lens = token_lens.to(opt.device).long()\n",
    "\n",
    "m_length = token_lens * 4\n",
    "captions = prompt_list\n",
    "\n",
    "sample = 0\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "converter = Joint2BVHConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bede8314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "Loading Transformer 2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd from epoch 1999!\n"
     ]
    }
   ],
   "source": [
    "t2m_transformer = load_trans_model(model_opt , opt ,'latest.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "feeabc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"the person runs in forward direction\",\n",
    "     \n",
    "]\n",
    "est_length = True\n",
    "length_list = []\n",
    "\n",
    "# if est_length:\n",
    "#     print(\"Since no motion length are specified, we will use estimated motion lengthes!!\")\n",
    "#     text_embedding = t2m_transformer.encode_text(prompt_list)\n",
    "#     pred_dis = length_estimator(text_embedding)\n",
    "#     probs = F.softmax(pred_dis, dim=-1)  # (b, ntoken)\n",
    "#     token_lens = Categorical(probs).sample()  # (b, seqlen)\n",
    "#     lengths = torch.multinomial()\n",
    "# else:\n",
    "token_lens = torch.LongTensor(length_list) // 4\n",
    "token_lens = token_lens.to(opt.device).long()\n",
    "\n",
    "m_length = token_lens * 4\n",
    "captions = prompt_list\n",
    "\n",
    "sample = 0\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "converter = Joint2BVHConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4c2942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "t2m_transformer = t2m_transformer.float()\n",
    "res_model = res_model.float()\n",
    "vq_model = vq_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "21641ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_transform(data):\n",
    "       return data * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mids, pred_len = t2m_transformer.generate(captions, token_lens,\n",
    "                                    timesteps=opt.time_steps,\n",
    "                                    cond_scale=opt.cond_scale,\n",
    "                                    temperature=opt.temperature,\n",
    "                                    topk_filter_thres=opt.topkr,\n",
    "                                    gsample=opt.gumbel_sample,\n",
    "                                    is_predict_len=opt.motion_length==-1\n",
    "                                    )\n",
    "    token_lens = pred_len\n",
    "    m_length = token_lens*4\n",
    "    # print(mids)\n",
    "    # print(mids.shape)\n",
    "    mids_1 = res_model.generate(mids, captions, token_lens, temperature=1, cond_scale=5)\n",
    "    pred_motions = vq_model.forward_decoder(mids_1)\n",
    "    pred_motions_1 = pred_motions.detach().numpy()\n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths - A: 40, B: 50, C: 8\n",
      "Using half_token_length: 8\n",
      "Token shape: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract parts from quantized\n",
    "quantized_base = quantized[0, :]  # Shape: [L, 6]\n",
    "part1 = quantized_base[:insert_frame]        # Quantized before 10s\n",
    "part2 = quantized_base[insert_frame:]        # Quantized after 10s\n",
    "\n",
    "\n",
    "motion_A = part1[:, 0]        # Last tokens before 10s\n",
    "motion_B = mids_1[0, : , 0]     # New motion to insert\n",
    "motion_C = part2[:, 0]        # Tokens after 10s\n",
    "\n",
    "# Calculate actual lengths\n",
    "len_A = len(motion_A)\n",
    "len_B = len(motion_B)\n",
    "len_C = len(motion_C)\n",
    "\n",
    "# Adjust half_token_length based on minimum available length\n",
    "half_token_length = min(24, len_A, len_B, len_C)\n",
    "num_transition_token = 2\n",
    "\n",
    "# Create tokens tensor with appropriate size\n",
    "tokens = -1 * torch.ones((2, half_token_length*2 + num_transition_token), dtype=torch.long, device=opt.device)\n",
    "\n",
    "# === Transition 1: part1 (A) to mids (B) ===\n",
    "tokens[0, :half_token_length] = motion_A[-half_token_length:]\n",
    "tokens[0, half_token_length:half_token_length + num_transition_token] = t2m_transformer.pad_id\n",
    "tokens[0, half_token_length + num_transition_token:] = motion_B[:half_token_length]\n",
    "\n",
    "# === Transition 2: mids (B) to part2 (C) ===\n",
    "tokens[1, :half_token_length] = motion_B[-half_token_length:]\n",
    "tokens[1, half_token_length:half_token_length + num_transition_token] = t2m_transformer.pad_id\n",
    "tokens[1, half_token_length + num_transition_token:] = motion_C[:half_token_length]\n",
    "\n",
    "# Debug prints\n",
    "print(f\"Lengths - A: {len_A}, B: {len_B}, C: {len_C}\")\n",
    "print(f\"Using half_token_length: {half_token_length}\")\n",
    "print(f\"Token shape: {tokens.shape}\")\n",
    "z\n",
    "# Predict transitions\n",
    "tokens[tokens == -1] = t2m_transformer.pad_id\n",
    "inpainting_mask = tokens == t2m_transformer.pad_id\n",
    "inpaint_index = t2m_transformer.edit2(None, tokens)\n",
    "\n",
    "# Extract transitions\n",
    "transition1 = inpaint_index[0, inpainting_mask[0]].unsqueeze(-1)\n",
    "transition1 = torch.nn.functional.pad(transition1, (0, 5), value=-1)\n",
    "\n",
    "transition2 = inpaint_index[1, inpainting_mask[1]].unsqueeze(-1)\n",
    "transition2 = torch.nn.functional.pad(transition2, (0, 5), value=-1)\n",
    "\n",
    "# Construct final motion\n",
    "final_motion = torch.cat([\n",
    "    part1,         # original motion up to 10s\n",
    "    transition1,   # transition: original → mids\n",
    "    mids_1[0][:pred_len[0]],       # inserted new motion\n",
    "    transition2,   # transition: mids → original continuation\n",
    "    part2          # rest of original motion\n",
    "], dim=0).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Final result\n",
    "quantized = final_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4062858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motions = vq_model.forward_decoder(mids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c8f1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inv_transform(pred_motions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6c49b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "combine = True  # Set to True if you want to combine the data, False otherwise\n",
    "def export_bvh(k, joint_data):\n",
    "            length = m_length[k]\n",
    "            animation_path = pjoin(animation_dir, str(k))\n",
    "            joint_path = pjoin(joints_dir, str(k))\n",
    "            os.makedirs(animation_path, exist_ok=True)\n",
    "            os.makedirs(joint_path, exist_ok=True)\n",
    "            joint_data = joint_data if combine else joint_data[:length]\n",
    "            joint = recover_from_ric(torch.from_numpy(joint_data).float().pin_memory().to(opt.device), 22).cpu().numpy()\n",
    "            bvh_path = pjoin(animation_path, f\"sample{k}_inmiddle_{length}_ik.bvh\")\n",
    "            converter.convert(joint, filename=bvh_path, iterations=50)\n",
    "            bvh_path = pjoin(animation_path, f\"sample{k}_inmiddle_len{length}.bvh\")\n",
    "            converter.convert(joint, filename=bvh_path, iterations=50, foot_ik=False)\n",
    "\n",
    "threads = [threading.Thread(target=export_bvh, args=(k, joint_data)) for k, joint_data in enumerate(data)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92199e85",
   "metadata": {},
   "source": [
    "do all the other processing same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAMM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
