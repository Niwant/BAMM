{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b93629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17375a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewfore decode you get tokens \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5246294",
   "metadata": {},
   "source": [
    "encode-> add motion in between -> decode\n",
    "encode the prev motion(bvh) -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from common.skeleton import Skeleton\n",
    "from utils.paramUtil import *\n",
    "from common.quaternion import qrot_np, qinv_np, qmul_np, quaternion_to_cont6d_np, qbetween_np, qfix\n",
    "\n",
    "def joints_to_features(joints, feet_thre=0.002, dataset='humanml'):\n",
    "    \"\"\"\n",
    "    joints: numpy array of shape (frames, joints_num, 3)\n",
    "    returns: RIC feature vector (frames-1, feature_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == 'humanml':\n",
    "        n_raw_offsets = torch.from_numpy(t2m_raw_offsets)\n",
    "        kinematic_chain = t2m_kinematic_chain\n",
    "        joints_num = 22\n",
    "        fid_r, fid_l = [8, 11], [7, 10]\n",
    "        face_joint_indx = [2, 1, 17, 16]\n",
    "    elif dataset == 'kit':\n",
    "        n_raw_offsets = torch.from_numpy(kit_raw_offsets)\n",
    "        kinematic_chain = kit_kinematic_chain\n",
    "        joints_num = 21\n",
    "        fid_r, fid_l = [14, 15], [19, 20]\n",
    "        face_joint_indx = [11, 16, 5, 8]\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset')\n",
    "\n",
    "    joints = torch.from_numpy(joints).float()\n",
    "\n",
    "    # Step 1: Build Skeleton\n",
    "    skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "\n",
    "    # Step 2: Uniform skeleton (optional, skipped if already processed)\n",
    "    tgt_offsets = skel.get_offsets_joints(joints[0])\n",
    "\n",
    "    # Step 3: Put on floor\n",
    "    floor_height = joints[:, :, 1].min()\n",
    "    joints[:, :, 1] -= floor_height\n",
    "\n",
    "    # Step 4: Face Z+\n",
    "    r_hip, l_hip, sdr_r, sdr_l = face_joint_indx\n",
    "    across1 = joints[0, r_hip] - joints[0, l_hip]\n",
    "    across2 = joints[0, sdr_r] - joints[0, sdr_l]\n",
    "    across = across1 + across2\n",
    "    across = across / torch.norm(across)\n",
    "    forward_init = torch.cross(torch.tensor([0.0, 1.0, 0.0]), across)\n",
    "    forward_init = forward_init / torch.norm(forward_init)\n",
    "    target = torch.tensor([0.0, 0.0, 1.0])\n",
    "    root_quat_init = qbetween_np(forward_init.numpy(), target.numpy())\n",
    "    root_quat_init = np.ones(joints.shape[:-1] + (4,)) * root_quat_init\n",
    "    joints = qrot_np(root_quat_init, joints.numpy())\n",
    "\n",
    "    global_positions = joints.copy()\n",
    "\n",
    "    # Step 5: Foot contact detection\n",
    "    def foot_detect(positions, thres):\n",
    "        velfactor = np.array([thres, thres])\n",
    "        feet_l_x = (positions[1:, fid_l, 0] - positions[:-1, fid_l, 0]) ** 2\n",
    "        feet_l_y = (positions[1:, fid_l, 1] - positions[:-1, fid_l, 1]) ** 2\n",
    "        feet_l_z = (positions[1:, fid_l, 2] - positions[:-1, fid_l, 2]) ** 2\n",
    "        feet_l = ((feet_l_x + feet_l_y + feet_l_z) < velfactor).astype(np.float32)\n",
    "\n",
    "        feet_r_x = (positions[1:, fid_r, 0] - positions[:-1, fid_r, 0]) ** 2\n",
    "        feet_r_y = (positions[1:, fid_r, 1] - positions[:-1, fid_r, 1]) ** 2\n",
    "        feet_r_z = (positions[1:, fid_r, 2] - positions[:-1, fid_r, 2]) ** 2\n",
    "        feet_r = ((feet_r_x + feet_r_y + feet_r_z) < velfactor).astype(np.float32)\n",
    "\n",
    "        return feet_l, feet_r\n",
    "\n",
    "    feet_l, feet_r = foot_detect(joints, feet_thre)\n",
    "\n",
    "    # Step 6: Extract features\n",
    "\n",
    "    # Inverse kinematics to quaternions\n",
    "    skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "    quat_params = skel.inverse_kinematics_np(joints, face_joint_indx, smooth_forward=True)\n",
    "    quat_params = qfix(quat_params)\n",
    "\n",
    "    # Continuous 6D\n",
    "    cont_6d_params = quaternion_to_cont6d_np(quat_params)\n",
    "    r_rot = quat_params[:, 0]\n",
    "\n",
    "    # Root linear velocities\n",
    "    velocity = joints[1:, 0] - joints[:-1, 0]\n",
    "    velocity = qrot_np(r_rot[1:], velocity)\n",
    "    l_velocity = velocity[:, [0, 2]]\n",
    "\n",
    "    # Root rotation velocities\n",
    "    r_velocity = qmul_np(r_rot[1:], qinv_np(r_rot[:-1]))\n",
    "    r_velocity = np.arcsin(r_velocity[:, 2:3])\n",
    "\n",
    "    # Root height\n",
    "    root_y = joints[:-1, 0, 1:2]\n",
    "\n",
    "    root_data = np.concatenate([r_velocity, l_velocity, root_y], axis=-1)\n",
    "\n",
    "    # Joint positions (RIC)\n",
    "    joints_local = joints.copy()\n",
    "    joints_local[..., 0] -= joints_local[:, 0:1, 0]\n",
    "    joints_local[..., 2] -= joints_local[:, 0:1, 2]\n",
    "    joints_local = qrot_np(np.repeat(r_rot[:, None], joints_local.shape[1], axis=1), joints_local)\n",
    "    ric_data = joints_local[:, 1:].reshape(joints_local.shape[0], -1)\n",
    "\n",
    "    # Joint rotations\n",
    "    rot_data = cont_6d_params[:, 1:].reshape(cont_6d_params.shape[0], -1)\n",
    "\n",
    "    # Joint velocities\n",
    "    local_vel = qrot_np(np.repeat(r_rot[:-1, None], global_positions.shape[1], axis=1),\n",
    "                        global_positions[1:] - global_positions[:-1])\n",
    "    local_vel = local_vel.reshape(local_vel.shape[0], -1)\n",
    "\n",
    "    # Combine\n",
    "    data = np.concatenate([root_data, ric_data[:-1], rot_data[:-1], local_vel, feet_l, feet_r], axis=-1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cb1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(r\"generation\\generation_name_nopredlen\\joints\\0\\sample0_repeat0_len192_ik.npy\", allow_pickle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb98d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 22, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd2405cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200, 263)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motions_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806ebc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467, 263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_20584\\4195675775.py:46: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Cross.cpp:66.)\n",
      "  forward_init = torch.cross(torch.tensor([0.0, 1.0, 0.0]), across)\n"
     ]
    }
   ],
   "source": [
    "ric_features = joints_to_features(test, feet_thre=0.002, dataset='humanml')\n",
    "\n",
    "print(ric_features.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e693c25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 22, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d69d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'mean.npy'))\n",
    "std = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'std.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f092ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_joints = (ric_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba2bfae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 263)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_joints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37775734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ric_tensor = torch.from_numpy(normed_joints).float().unsqueeze(0).to('cpu')  # (1, seq_len, 263)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64ce5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 467, 263])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ric_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6862106",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized, tokens = vq_model.encode(ric_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b786228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 263])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0635577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 467, 263])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ric_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93f602f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 6])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e13d8ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 116, 6])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efd7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion generation completed successfully! BVH files saved in ./generation\\generation_name_nopredlen\\animations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./generation\\\\generation_name_nopredlen\\\\animations\\\\0\\\\sample0_repeat0_len80.bvh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt=[\"A man is runnning\"]\n",
    "text_prompt=\"|\".join(test_prompt)\n",
    "generator.create_bvh_from_in(text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e197234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batch_size: 32\n",
      "checkpoints_dir: ./log/t2m\n",
      "cond_scale: 4\n",
      "dataset_name: t2m\n",
      "dropout: 0.2\n",
      "ext: text2motion\n",
      "ff_size: 1024\n",
      "force_mask: False\n",
      "gpu_id: -1\n",
      "gumbel_sample: False\n",
      "is_train: False\n",
      "latent_dim: 384\n",
      "mask_edit_section: None\n",
      "max_motion_length: 196\n",
      "motion_length: 0\n",
      "n_heads: 6\n",
      "n_layers: 8\n",
      "name: t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns\n",
      "num_batch: 2\n",
      "repeat_times: 1\n",
      "res_name: tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\n",
      "seed: 1\n",
      "source_motion: example_data/000612.npy\n",
      "temperature: 1.0\n",
      "text_path: \n",
      "text_prompt: \n",
      "time_steps: 10\n",
      "topkr: 0.9\n",
      "unit_length: 4\n",
      "use_res_model: False\n",
      "vq_name: rvq_nq1_dc512_nc512\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['']  # Prevent argparse from picking up unwanted Jupyter arguments\n",
    "\n",
    "# from gen_t2m import EvalT2MOptions  # or from wherever your file is\n",
    "\n",
    "# Initialize and parse\n",
    "parser = EvalT2MOptions()\n",
    "opt = parser.parse(is_eval=True)\n",
    "\n",
    "# Set values as per your CLI\n",
    "opt.res_name = \"tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\"\n",
    "opt.name = \"2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\"\n",
    "opt.text_prompt = \"the person crouches and walks forward.\"\n",
    "opt.motion_length = -1\n",
    "opt.repeat_times = 3\n",
    "opt.gpu_id = -1\n",
    "opt.seed = 1\n",
    "opt.ext = \"generation_name_nopredlen\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfe6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixseed(opt.seed)\n",
    "\n",
    "opt.device = torch.device(\"cpu\" if opt.gpu_id == -1 else \"cuda:\" + str(opt.gpu_id))\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "dim_pose = 251 if opt.dataset_name == 'kit' else 263\n",
    "\n",
    "# out_dir = pjoin(opt.check)\n",
    "root_dir = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)\n",
    "model_dir = pjoin(root_dir, 'model')\n",
    "result_dir = pjoin('./generation', opt.ext)\n",
    "joints_dir = pjoin(result_dir, 'joints')\n",
    "animation_dir = pjoin(result_dir, 'animations')\n",
    "os.makedirs(joints_dir, exist_ok=True)\n",
    "os.makedirs(animation_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c216c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./log/t2m\\t2m\\2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\\opt.txt\n"
     ]
    }
   ],
   "source": [
    "model_opt_path = pjoin(root_dir, 'opt.txt')\n",
    "model_opt = get_opt(model_opt_path, device=opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f1fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./log/vq\\t2m\\rvq_nq6_dc512_nc512_noshare_qdp0.2\\opt.txt\n",
      "Loading VQ Model rvq_nq6_dc512_nc512_noshare_qdp0.2 Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m_c.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pjoin(vq_opt.checkpoints_dir, vq_opt.dataset_name, vq_opt.name, 'model', 'net_best_fid.tar'),\n"
     ]
    }
   ],
   "source": [
    "global vq_opt # This now works as vq_opt is global\n",
    "\n",
    "vq_opt_path = pjoin('./log/vq', opt.dataset_name, model_opt.vq_name, 'opt.txt')\n",
    "vq_opt = get_opt(vq_opt_path, device=opt.device)\n",
    "vq_model, vq_opt = load_vq_model(vq_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c967763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.num_tokens = vq_opt.nb_code\n",
    "model_opt.num_quantizers = vq_opt.num_quantizers\n",
    "model_opt.code_dim = vq_opt.code_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25fbee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\t2m\\\\tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\\\\opt.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjoin('checkpoints', opt.dataset_name, opt.res_name, 'opt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726b36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_res_model(res_opt,vq_opt):\n",
    "    res_opt.num_quantizers = vq_opt.num_quantizers\n",
    "    res_opt.num_tokens = vq_opt.nb_code\n",
    "    res_transformer = ResidualTransformer(code_dim=vq_opt.code_dim,\n",
    "                                            cond_mode='text',\n",
    "                                            latent_dim=res_opt.latent_dim,\n",
    "                                            ff_size=res_opt.ff_size,\n",
    "                                            num_layers=res_opt.n_layers,\n",
    "                                            num_heads=res_opt.n_heads,\n",
    "                                            dropout=res_opt.dropout,\n",
    "                                            clip_dim=512,\n",
    "                                            shared_codebook=vq_opt.shared_codebook,\n",
    "                                            cond_drop_prob=res_opt.cond_drop_prob,\n",
    "                                            # codebook=vq_model.quantizer.codebooks[0] if opt.fix_token_emb else None,\n",
    "                                            share_weight=res_opt.share_weight,\n",
    "                                            clip_version=clip_version,\n",
    "                                            opt=res_opt)\n",
    "\n",
    "    ckpt = torch.load(pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                      map_location=opt.device)\n",
    "    missing_keys, unexpected_keys = res_transformer.load_state_dict(ckpt['res_transformer'], strict=False)\n",
    "    assert len(unexpected_keys) == 0\n",
    "    assert all([k.startswith('clip_model.') for k in missing_keys])\n",
    "    print(f'Loading Residual Transformer {res_opt.name} from epoch {ckpt[\"ep\"]}!')\n",
    "    return res_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e2fe055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints\\t2m\\tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\\opt.txt\n",
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n",
      "Loading CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Residual Transformer tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw from epoch 440!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_20584\\2724606168.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar'),\n"
     ]
    }
   ],
   "source": [
    "res_opt_path = pjoin('checkpoints', opt.dataset_name, opt.res_name, 'opt.txt')\n",
    "res_opt = get_opt(res_opt_path, device=opt.device)\n",
    "res_model = load_res_model(res_opt,vq_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd38549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints\\\\t2m\\\\tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\\\\model\\\\net_best_fid.tar'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bd3c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./log/t2m\\\\t2m\\\\2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\\\\model\\\\latest.tar'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pjoin(model_opt.checkpoints_dir, model_opt.dataset_name, model_opt.name, 'model', 'latest.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f780fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert res_opt.vq_name == model_opt.vq_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e07f64aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n",
      "Loading CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m_c.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pjoin(model_opt.checkpoints_dir, model_opt.dataset_name, model_opt.name, 'model', which_model),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Transformer 2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd from epoch 1999!\n",
      "Loading Length Estimator from epoch 11!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m_c.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pjoin('checkpoints', opt.dataset_name, 'length_estimator', 'model', 'finest.tar'),\n"
     ]
    }
   ],
   "source": [
    "t2m_transformer = load_trans_model(model_opt, opt, 'latest.tar')\n",
    "\n",
    "##################################\n",
    "#####Loading Length Predictor#####\n",
    "##################################\n",
    "length_estimator = load_len_estimator(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "886bfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_transformer.eval()\n",
    "vq_model.eval()\n",
    "res_model.eval()\n",
    "length_estimator.eval()\n",
    "\n",
    "res_model.to(opt.device)\n",
    "t2m_transformer.to(opt.device)\n",
    "vq_model.to(opt.device)\n",
    "length_estimator.to(opt.device)\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "opt.nb_joints = 21 if opt.dataset_name == 'kit' else 22\n",
    "\n",
    "mean = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'mean.npy'))\n",
    "std = np.load(pjoin('checkpoints', opt.dataset_name, model_opt.vq_name, 'meta', 'std.npy'))\n",
    "def inv_transform(data):\n",
    "    return data * std + mean\n",
    "\n",
    "prompt_list = []\n",
    "length_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "422244dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    \"the person runs in forward direction\",\n",
    "     \n",
    "]\n",
    "est_length = True\n",
    "length_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261089e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since no motion length are specified, we will use estimated motion lengthes!!\n"
     ]
    }
   ],
   "source": [
    "if est_length:\n",
    "    print(\"Since no motion length are specified, we will use estimated motion lengthes!!\")\n",
    "    text_embedding = t2m_transformer.encode_text(prompt_list)\n",
    "    pred_dis = length_estimator(text_embedding)\n",
    "    probs = F.softmax(pred_dis, dim=-1)  # (b, ntoken)\n",
    "    token_lens = Categorical(probs).sample()  # (b, seqlen)\n",
    "    # lengths = torch.multinomial()\n",
    "else:\n",
    "    token_lens = torch.LongTensor(length_list) // 4\n",
    "    token_lens = token_lens.to(opt.device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_length = token_lens * 4\n",
    "captions = prompt_list\n",
    "\n",
    "sample = 0\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "converter = Joint2BVHConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5143548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mids, pred_len = t2m_transformer.generate(captions, token_lens,\n",
    "                                    timesteps=opt.time_steps,\n",
    "                                    cond_scale=opt.cond_scale,\n",
    "                                    temperature=opt.temperature,\n",
    "                                    topk_filter_thres=opt.topkr,\n",
    "                                    gsample=opt.gumbel_sample,\n",
    "                                    is_predict_len=opt.motion_length==-1)\n",
    "    token_lens = pred_len\n",
    "    m_length = token_lens*4\n",
    "    # print(mids)\n",
    "    # print(mids.shape)\n",
    "    mids_1 = res_model.generate(mids, captions, token_lens, temperature=1, cond_scale=5)\n",
    "    pred_motions = vq_model.forward_decoder(mids_1)\n",
    "    pred_motions_1 = pred_motions.detach().cpu().numpy()\n",
    "             \n",
    "    data = inv_transform(pred_motions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c388ecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 6])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mids_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54bf03f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 116, 6])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transition_token = 2\n",
    "b = mids.shape[0]\n",
    "half_token_length = (pred_len / 2).int()\n",
    "idx_full_len = half_token_length >= 24\n",
    "half_token_length[idx_full_len] -= 1\n",
    "\n",
    "# tokens = -1 * torch.ones((b - 1, 50), dtype=torch.long)\n",
    "tokens = -1 * torch.ones((b - 1, 50), dtype=torch.long, device=opt.device)\n",
    "\n",
    "transition_train_length = []\n",
    "for i in range(b - 1):\n",
    "    i_index_motion = mids[i, :, 0]\n",
    "    i1_index_motion = mids[i + 1, :, 0]\n",
    "\n",
    "    left_end = half_token_length[i]\n",
    "    right_start = left_end + num_transition_token\n",
    "    end = right_start + half_token_length[i + 1]\n",
    "\n",
    "    tokens[i, :left_end] = i_index_motion[pred_len[i] - left_end: pred_len[i]]\n",
    "    tokens[i, left_end:right_start] = t2m_transformer.pad_id\n",
    "    tokens[i, right_start:end] = i1_index_motion[:half_token_length[i + 1]]\n",
    "    transition_train_length.append(end)\n",
    "\n",
    "transition_train_length = torch.tensor(transition_train_length).to(mids.device).long()\n",
    "tokens = tokens.scatter_(-1, transition_train_length[..., None], t2m_transformer.end_id).long()\n",
    "inpainting_mask = tokens == t2m_transformer.pad_id\n",
    "tokens[tokens == -1] = t2m_transformer.pad_id\n",
    "inpaint_index = t2m_transformer.edit2(None, tokens)\n",
    "\n",
    "all_tokens = []\n",
    "for i in range(b - 1):\n",
    "    all_tokens.append(mids[i, :pred_len[i]])\n",
    "    inpaint_all = inpaint_index[i, inpainting_mask[i]].unsqueeze(-1)\n",
    "    inpaint_all = torch.nn.functional.pad(inpaint_all, (0, 5), value=-1)\n",
    "    all_tokens.append(inpaint_all)\n",
    "all_tokens.append(mids[-1, :pred_len[-1]])\n",
    "mids = torch.cat(all_tokens).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d115d",
   "metadata": {},
   "source": [
    "Inbetween code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 20\n",
    "t_sec=10 # addtion at this second\n",
    "downsample_factor=5\n",
    "frame_index = int(t_sec * FPS)\n",
    "insert_frame = int(frame_index / downsample_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7254782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e9948c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 6])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mids_1[0][:pred_len[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "28526f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract parts from quantized\n",
    "quantized_base = quantized[0, :]  # Shape: [L, 6]\n",
    "part1 = quantized_base[:insert_frame]        # Quantized before 10s\n",
    "part2 = quantized_base[insert_frame:]        # Quantized after 10s\n",
    "\n",
    "\n",
    "motion_A = part1[:, 0]        # Last tokens before 10s\n",
    "motion_B = mids_1[0, : , 0]     # New motion to insert\n",
    "motion_C = part2[:, 0]        # Tokens after 10s\n",
    "\n",
    "# Prepare transition tokens\n",
    "tokens = -1 * torch.ones((2, 50), dtype=torch.long, device=opt.device)\n",
    "half_token_length = 24\n",
    "num_transition_token = 2\n",
    "\n",
    "# === Transition 1: part1 (A) to mids (B) ===\n",
    "tokens[0, :half_token_length] = motion_A[-half_token_length:]\n",
    "tokens[0, half_token_length:half_token_length + num_transition_token] = t2m_transformer.pad_id\n",
    "tokens[0, half_token_length + num_transition_token:] = motion_B[:half_token_length]\n",
    "\n",
    "# === Transition 2: mids (B) to part2 (C) ===\n",
    "tokens[1, :half_token_length] = motion_B[-half_token_length:]\n",
    "tokens[1, half_token_length:half_token_length + num_transition_token] = t2m_transformer.pad_id\n",
    "tokens[1, half_token_length + num_transition_token:] = motion_C[:half_token_length]\n",
    "\n",
    "# Predict transitions\n",
    "tokens[tokens == -1] = t2m_transformer.pad_id\n",
    "inpainting_mask = tokens == t2m_transformer.pad_id\n",
    "inpaint_index = t2m_transformer.edit2(None, tokens)\n",
    "\n",
    "# Extract transitions\n",
    "transition1 = inpaint_index[0, inpainting_mask[0]].unsqueeze(-1)\n",
    "transition1 = torch.nn.functional.pad(transition1, (0, 5), value=-1)\n",
    "\n",
    "transition2 = inpaint_index[1, inpainting_mask[1]].unsqueeze(-1)\n",
    "transition2 = torch.nn.functional.pad(transition2, (0, 5), value=-1)\n",
    "\n",
    "# Construct final motion\n",
    "final_motion = torch.cat([\n",
    "    part1,         # original motion up to 10s\n",
    "    transition1,   # transition: original → mids\n",
    "    mids_1[0][:pred_len[0]],       # inserted new motion\n",
    "    transition2,   # transition: mids → original continuation\n",
    "    part2          # rest of original motion\n",
    "], dim=0).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Final result\n",
    "quantized = final_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9265baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 263])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c62068fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mids_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5cd7a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 313, 6])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "60d54a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motions = vq_model.forward_decoder(mids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "029ba09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motions_2 =pred_motions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14758f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = inv_transform(pred_motions_2)\n",
    "joint2 = recover_from_ric(torch.from_numpy(data2).float(), 22).numpy()\n",
    "_, ik_joint = converter.convert(joint2[0], filename=\"inframe1.bvh\", iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5e81408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200, 263)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ecd7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial1=vq_model.forward_decoder(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2e67712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motions_1 = trial1.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf2d916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inv_transform(pred_motions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f0a079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = recover_from_ric(torch.from_numpy(data).float(), 22).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0804f0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200, 22, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da182a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "_, ik_joint = converter.convert(joint[0], filename=\"inframe.bvh\", iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dc442efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 263)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cdc92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    'a figure sits on the chair.',\n",
    "            #  'a person squatting raises both their arms above their head.',\n",
    "             'a man jump forward.',\n",
    "             'a person punches as if they are boxing.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81098ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_token_length = torch.ceil((pred_len)/4).int()\n",
    "b=len(prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e83f1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_token_length = (m_token_length/2).int()\n",
    "idx_full_len = half_token_length >= 24\n",
    "half_token_length[idx_full_len] = half_token_length[idx_full_len] - 1\n",
    "\n",
    "mask_id = 512\n",
    "tokens = -1*torch.ones((b-1, 50), dtype=torch.long)\n",
    "transition_train_length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6c7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c7fe27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transition_token=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8caa2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(b-1):\n",
    "    i_index_motion = mids[i]\n",
    "    i1_index_motion = mids[i+1]\n",
    "\n",
    "    left_end = half_token_length[i]\n",
    "    right_start = left_end + num_transition_token\n",
    "    end = right_start + half_token_length[i+1]\n",
    "\n",
    "    tokens[i, :left_end] = i_index_motion[m_token_length[i]-left_end: m_token_length[i]]\n",
    "    tokens[i, left_end:right_start] = mask_id\n",
    "    tokens[i, right_start:end] = i1_index_motion[:half_token_length[i+1]]\n",
    "    transition_train_length.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2b7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fe1d70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_train_length = torch.tensor(transition_train_length).to(mids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3680cc11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 513",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ans\u001b[38;5;241m=\u001b[39m\u001b[43mres_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_train_length\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\models\\mask_transformer\\tools.py:40\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 40\u001b[0m out \u001b[38;5;241m=\u001b[39m fn(model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\models\\mask_transformer\\transformer.py:1059\u001b[0m, in \u001b[0;36mResidualTransformer.generate\u001b[1;34m(self, motion_ids, conds, m_lens, temperature, topk_filter_thres, cond_scale, num_res_layers, force_mask)\u001b[0m\n\u001b[0;32m   1057\u001b[0m token_embed \u001b[38;5;241m=\u001b[39m repeat(token_embed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc d -> b c d\u001b[39m\u001b[38;5;124m'\u001b[39m, b\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m   1058\u001b[0m gathered_ids \u001b[38;5;241m=\u001b[39m repeat(motion_ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n -> b n d\u001b[39m\u001b[38;5;124m'\u001b[39m, d\u001b[38;5;241m=\u001b[39mtoken_embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m-> 1059\u001b[0m history_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtoken_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgathered_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_with_cond_scale(history_sum, i, cond_vector, padding_mask, cond_scale\u001b[38;5;241m=\u001b[39mcond_scale, force_mask\u001b[38;5;241m=\u001b[39mforce_mask)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# logits = self.trans_forward(history_sum, qids, cond_vector, padding_mask)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 513"
     ]
    }
   ],
   "source": [
    "ans=res_model.generate(tokens, captions[:-1], transition_train_length*4, temperature=1, cond_scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d7120299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import smplx\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d35619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "from smplx import SMPL\n",
    "\n",
    "model = SMPL(\n",
    "    model_path='smplx/smpl',\n",
    "    gender='NEUTRAL',   # or 'MALE' / 'FEMALE'\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f9308398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ballet/smpl/000000.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Assuming single person\n",
    "person = data[0]\n",
    "Rh = torch.tensor(person[\"Rh\"], dtype=torch.float32)  # (1, 3)\n",
    "Th = torch.tensor(person[\"Th\"], dtype=torch.float32)  # (1, 3)\n",
    "poses = torch.tensor(person[\"poses\"], dtype=torch.float32).reshape(1, -1)  # (1, 72)\n",
    "shapes = torch.tensor(person[\"shapes\"], dtype=torch.float32)  # (1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6382c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "person = data[0]\n",
    "Rh = torch.tensor(person[\"Rh\"], dtype=torch.float32)         # (1, 3)\n",
    "Th = torch.tensor(person[\"Th\"], dtype=torch.float32)         # (1, 3)\n",
    "poses = torch.tensor(person[\"poses\"], dtype=torch.float32)   # (1, 69)\n",
    "shapes = torch.tensor(person[\"shapes\"], dtype=torch.float32) # (1, 10)\n",
    "\n",
    "# Use Rh as global_orient, and the rest as body_pose\n",
    "output = model(\n",
    "    global_orient=Rh,\n",
    "    body_pose=poses,  # Skip the first 3 (which are 0s)\n",
    "    betas=shapes,\n",
    "    transl=Th\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5aa4aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses[0][:3]=Rh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bfb8a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8230, -0.1150,  0.2800, -0.1530, -0.0690, -0.0180,  0.1640,  0.1160,\n",
       "        -0.0530,  0.2290,  0.0750, -0.1170,  0.8130, -0.1400,  0.1070,  0.1270,\n",
       "        -0.0370,  0.0240, -0.1190,  0.1750,  0.1130, -0.0540, -0.1430,  0.0740,\n",
       "        -0.0890,  0.0560,  0.0060, -0.0840,  0.1610,  0.0650,  0.0750,  0.0340,\n",
       "        -0.0610,  0.0880,  0.1190, -0.0550, -0.0280, -0.1570, -0.3400,  0.0520,\n",
       "         0.3480,  0.4080, -0.0250,  0.0070,  0.0170,  0.3600, -0.2150, -0.9730,\n",
       "         0.3310,  0.2810,  0.9410,  0.3270, -0.8880,  0.1240,  0.3170,  1.0230,\n",
       "        -0.0950,  0.0830, -0.1700, -0.2930, -0.1420,  0.1970,  0.2050, -0.2890,\n",
       "        -0.1310, -0.3500, -0.0880,  0.1180,  0.3370])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ed31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1073, -1.1542,  0.6699],\n",
       "         [ 0.1720, -1.0624,  0.6915],\n",
       "         [ 0.0378, -1.0773,  0.7134],\n",
       "         [ 0.1154, -1.2644,  0.6618],\n",
       "         [ 0.1808, -0.6969,  0.6190],\n",
       "         [-0.0348, -0.7124,  0.7923],\n",
       "         [ 0.1308, -1.3818,  0.5992],\n",
       "         [ 0.1200, -0.3121,  0.6868],\n",
       "         [ 0.0946, -0.5091,  1.1063],\n",
       "         [ 0.1306, -1.4104,  0.5485],\n",
       "         [ 0.1467, -0.2639,  0.5659],\n",
       "         [ 0.0312, -0.3927,  1.0935],\n",
       "         [ 0.1405, -1.6162,  0.4825],\n",
       "         [ 0.2162, -1.5260,  0.5150],\n",
       "         [ 0.0583, -1.5381,  0.5287],\n",
       "         [ 0.1529, -1.6397,  0.4047],\n",
       "         [ 0.3105, -1.5145,  0.5057],\n",
       "         [-0.0407, -1.5260,  0.5156],\n",
       "         [ 0.3624, -1.2926,  0.6279],\n",
       "         [-0.0922, -1.3041,  0.6240],\n",
       "         [ 0.2776, -1.0754,  0.5484],\n",
       "         [ 0.0235, -1.0999,  0.5309],\n",
       "         [ 0.2325, -1.0038,  0.5523],\n",
       "         [ 0.0812, -1.0393,  0.5189],\n",
       "         [ 0.1569, -1.6355,  0.2892],\n",
       "         [ 0.1250, -1.6862,  0.2994],\n",
       "         [ 0.1917, -1.6841,  0.3030],\n",
       "         [ 0.0800, -1.7152,  0.3890],\n",
       "         [ 0.2270, -1.7098,  0.3953],\n",
       "         [ 0.1266, -0.2700,  0.4900],\n",
       "         [ 0.1888, -0.2588,  0.5435],\n",
       "         [ 0.1246, -0.2759,  0.7437],\n",
       "         [ 0.0361, -0.3300,  1.0459],\n",
       "         [-0.0062, -0.3734,  1.1084],\n",
       "         [ 0.1271, -0.5258,  1.1656],\n",
       "         [ 0.1450, -1.0436,  0.5277],\n",
       "         [ 0.1383, -0.9690,  0.5440],\n",
       "         [ 0.1507, -0.9417,  0.5609],\n",
       "         [ 0.1759, -0.9392,  0.5749],\n",
       "         [ 0.2122, -0.9485,  0.5906],\n",
       "         [ 0.1501, -1.1080,  0.4859],\n",
       "         [ 0.1732, -1.0228,  0.5013],\n",
       "         [ 0.1650, -0.9914,  0.5145],\n",
       "         [ 0.1416, -0.9793,  0.5297],\n",
       "         [ 0.1057, -0.9798,  0.5505]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1db81897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb581f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=file[0]\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f67727b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses=[i for i in reversed(data['poses'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3d91c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.466,\n",
       "  -0.027,\n",
       "  0.1,\n",
       "  -0.241,\n",
       "  -0.064,\n",
       "  -0.027,\n",
       "  0.179,\n",
       "  0.09,\n",
       "  -0.057,\n",
       "  0.233,\n",
       "  0.059,\n",
       "  -0.115,\n",
       "  0.907,\n",
       "  -0.153,\n",
       "  0.129,\n",
       "  0.114,\n",
       "  -0.046,\n",
       "  0.02,\n",
       "  -0.112,\n",
       "  0.176,\n",
       "  0.113,\n",
       "  -0.048,\n",
       "  -0.135,\n",
       "  0.075,\n",
       "  -0.091,\n",
       "  0.043,\n",
       "  0.006,\n",
       "  -0.092,\n",
       "  0.169,\n",
       "  0.056,\n",
       "  0.06,\n",
       "  0.021,\n",
       "  -0.079,\n",
       "  0.086,\n",
       "  0.126,\n",
       "  -0.083,\n",
       "  -0.03,\n",
       "  -0.21,\n",
       "  -0.359,\n",
       "  0.041,\n",
       "  0.343,\n",
       "  0.408,\n",
       "  -0.028,\n",
       "  0.034,\n",
       "  0.028,\n",
       "  0.352,\n",
       "  -0.247,\n",
       "  -0.977,\n",
       "  0.322,\n",
       "  0.246,\n",
       "  0.959,\n",
       "  0.328,\n",
       "  -0.831,\n",
       "  0.123,\n",
       "  0.299,\n",
       "  0.987,\n",
       "  -0.108,\n",
       "  0.102,\n",
       "  -0.161,\n",
       "  -0.285,\n",
       "  -0.133,\n",
       "  0.187,\n",
       "  0.208,\n",
       "  -0.286,\n",
       "  -0.132,\n",
       "  -0.347,\n",
       "  -0.092,\n",
       "  0.121,\n",
       "  0.357]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2bbdec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array([output.joints.numpy()[0],output.joints.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99869f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebc554b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a511743",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints=[output.joints[0],output.joints[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_7456\\2736449939.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  np.array(joints).shape\n",
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_7456\\2736449939.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(joints).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf3207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.joints2bvh import Joint2BVHConvertor\n",
    "converter = Joint2BVHConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "875b3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84de6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=np.load('joints.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "61795975",
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_to_smpl_idx = [\n",
    "    8,   # Pelvis (MidHip)\n",
    "    12,  # L_Hip\n",
    "    9,   # R_Hip\n",
    "    1,   # Spine1 (Neck as proxy)\n",
    "    13,  # L_Knee\n",
    "    10,  # R_Knee\n",
    "    0,   # Spine2 (Nose as proxy)\n",
    "    14,  # L_Ankle\n",
    "    11,  # R_Ankle\n",
    "    1,   # Spine3 (Neck)\n",
    "    19,  # L_Foot (LBigToe)\n",
    "    22,  # R_Foot (RBigToe)\n",
    "    1,   # Neck\n",
    "    5,   # L_Collar\n",
    "    2,   # R_Collar\n",
    "    0,   # Head (Nose)\n",
    "    5,   # L_Shoulder\n",
    "    2,   # R_Shoulder\n",
    "    6,   # L_Elbow\n",
    "    3,   # R_Elbow\n",
    "    7,   # L_Wrist\n",
    "    4,   # R_Wrist\n",
    "    7,   # L_Hand (approx: same as wrist)\n",
    "    4,   # R_Hand (approx: same as wrist)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ec797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5941bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "420f0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_joints = arr[:, openpose_to_smpl_idx, :]  # shape: (N, 24, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "444604b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip Z to fix orientation\n",
    "smpl_joints[..., 2] *= -1\n",
    "\n",
    "# Optional: normalize position around pelvis\n",
    "smpl_joints -= smpl_joints[:, 0:1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b4bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df2794e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808, 127, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a2deb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "arr=[]\n",
    "for i in os.listdir('jazz/keypoints3d')[:500]:\n",
    "    with open(os.path.join('jazz/keypoints3d',i),'r') as f:\n",
    "\n",
    "        file=json.load(f)\n",
    "        joints=file[0]['keypoints3d']\n",
    "        arr.append(joints)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "350cb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('ballet/smpl','000000.json'),'r') as f:\n",
    "\n",
    "        file=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49bd7556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'id': 0, 'Rh': [[-2.823, -0.115, 0.28]], 'Th': [[0.109, -0.933, 0.642]], 'poses': [[-0.49, -0.036, 0.086, -0.153, -0.069, -0.018, 0.164, 0.116, -0.053, 0.229, 0.075, -0.117, 0.813, -0.14, 0.107, 0.127, -0.037, 0.024, -0.119, 0.175, 0.113, -0.054, -0.143, 0.074, -0.089, 0.056, 0.006, -0.084, 0.161, 0.065, 0.075, 0.034, -0.061, 0.088, 0.119, -0.055, -0.028, -0.157, -0.34, 0.052, 0.348, 0.408, -0.025, 0.007, 0.017, 0.36, -0.215, -0.973, 0.331, 0.281, 0.941, 0.327, -0.888, 0.124, 0.317, 1.023, -0.095, 0.083, -0.17, -0.293, -0.142, 0.197, 0.205, -0.289, -0.131, -0.35, -0.088, 0.118, 0.337]], 'shapes': [[-0.177, -0.016, -0.004, 0.039, -0.003, 0.003, -0.001, 0.003, 0.0, -0.002]]},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "110143ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=np.load('ballet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd5cf962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8372, 127, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a14cbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ecff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from visualization.joints2bvh import Joint2BVHConvertor\n",
    "converter = Joint2BVHConvertor()\n",
    "# new_bvh=converter.convert(arr, \"ballet.bvh\", iterations=100, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5d5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ik_joint = converter.convert(temp, \"try_new.bvh\", iterations=100, fps=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c73340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ae7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = temp[\"poses\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051459ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = torch.from_numpy(temp[\"poses\"][:n]).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae27ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1808, 165])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40073989",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BVHGenerator' from 'gen_t2m' (c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmiddleware\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CORSMiddleware\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_t2m\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BVHGenerator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Body\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BVHGenerator' from 'gen_t2m' (c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m.py)"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Query, HTTPException\n",
    "from fastapi.responses import FileResponse\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from gen_t2m import BVHGenerator\n",
    "from fastapi import Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf452ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BVHGenerator' from 'gen_t2m' (c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_t2m\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BVHGenerator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BVHGenerator' from 'gen_t2m' (c:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m.py)"
     ]
    }
   ],
   "source": [
    "from gen_t2m import BVHGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7d957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\venv\\\\Scripts\\\\python.exe', 'gen_t2m.py', '--res_name', 'tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw', '--name', '2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd', '--text_prompt', '', '--motion_length', '-1', '--repeat_times', '1', '--gpu_id', '-1', '--seed', '1', '--ext', 'generation_name_nopredlen']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\venv\\\\Scripts\\\\python.exe', 'gen_t2m.py', '--res_name', 'tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw', '--name', '2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd', '--text_prompt', '', '--motion_length', '-1', '--repeat_times', '1', '--gpu_id', '-1', '--seed', '1', '--ext', 'generation_name_nopredlen'], returncode=0, stdout='------------ Options -------------\\nbatch_size: 32\\ncheckpoints_dir: ./log/t2m\\ncond_scale: 4\\ndataset_name: t2m\\ndropout: 0.2\\next: generation_name_nopredlen\\nff_size: 1024\\nforce_mask: False\\ngpu_id: -1\\ngumbel_sample: False\\nis_train: False\\nlatent_dim: 384\\nmask_edit_section: None\\nmax_motion_length: 196\\nmotion_length: -1\\nn_heads: 6\\nn_layers: 8\\nname: 2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\\nnum_batch: 2\\nrepeat_times: 1\\nres_name: tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\\nseed: 1\\nsource_motion: example_data/000612.npy\\ntemperature: 1.0\\ntext_path: \\ntext_prompt: \\ntime_steps: 10\\ntopkr: 0.9\\nunit_length: 4\\nuse_res_model: False\\nvq_name: rvq_nq1_dc512_nc512\\nwhich_epoch: latest\\n-------------- End ----------------\\nReading ./log/t2m\\\\t2m\\\\2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\\\\opt.txt\\nReading ./log/vq\\\\t2m\\\\rvq_nq6_dc512_nc512_noshare_qdp0.2\\\\opt.txt\\nLoading VQ Model rvq_nq6_dc512_nc512_noshare_qdp0.2 Completed!\\nReading checkpoints\\\\t2m\\\\tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\\\\opt.txt\\nlatent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\\nLoading CLIP...\\nLoading Residual Transformer tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw from epoch 440!\\nlatent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\\nLoading CLIP...\\nLoading Transformer 2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd from epoch 1999!\\nLoading Length Estimator from epoch 11!\\n', stderr=\"c:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\gen_t2m.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\\n  ckpt = torch.load(pjoin(vq_opt.checkpoints_dir, vq_opt.dataset_name, vq_opt.name, 'model', 'net_best_fid.tar'),\\nc:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\venv\\\\lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\\n  warnings.warn(\\nc:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\gen_t2m.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\\n  ckpt = torch.load(pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar'),\\nc:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\gen_t2m.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\\n  ckpt = torch.load(pjoin(model_opt.checkpoints_dir, model_opt.dataset_name, model_opt.name, 'model', which_model),\\nc:\\\\Users\\\\TP\\\\Desktop\\\\genAI\\\\BAMM\\\\gen_t2m.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\\n  ckpt = torch.load(pjoin('checkpoints', opt.dataset_name, 'length_estimator', 'model', 'finest.tar'),\\n\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_name: str = \"tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw\"\n",
    "name: str = \"2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd\"\n",
    "text_prompt: str=\"\"\n",
    "motion_length: int = -1\n",
    "repeat_times: int = 1\n",
    "gpu_id: int = -1\n",
    "seed: int = 1\n",
    "ext: str = \"generation_name_nopredlen\"\n",
    "\n",
    "command = [\n",
    "         sys.executable, \"gen_t2m.py\",\n",
    "        \"--res_name\", res_name,\n",
    "        \"--name\", name,\n",
    "        \"--text_prompt\", text_prompt,\n",
    "        \"--motion_length\", str(motion_length),\n",
    "        \"--repeat_times\", str(repeat_times),\n",
    "        \"--gpu_id\", str(gpu_id),\n",
    "        \"--seed\", str(seed),\n",
    "        \"--ext\", ext\n",
    "    ]\n",
    "print(command)\n",
    "subprocess.run(command, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try=BVHGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe201ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9484ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt=\"|\".join(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddb119f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_bvh_from_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\gen_t2m.py:180\u001b[0m, in \u001b[0;36mcreate_bvh_from_in\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_bvh_from_in\u001b[39m(text):\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mopt\u001b[49m)\n\u001b[0;32m    181\u001b[0m         opt\u001b[38;5;241m.\u001b[39mtext_prompt\u001b[38;5;241m=\u001b[39mtext\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    182\u001b[0m         prompt_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "res=create_bvh_from_in(text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b775c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "from smplx import create\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414dd4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "from smplx import create\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_path = './smplx'  # path to folder containing SMPL_NEUTRAL.pkl\n",
    "model = create(model_path, model_type='smpl', gender='neutral')\n",
    "\n",
    "# Load a sample file\n",
    "with open('ballet/smpl/000000.json') as f:\n",
    "    smpl_data = json.load(f)[0]\n",
    "\n",
    "#\n",
    "# Convert to tensors\n",
    "global_orient = torch.tensor(smpl_data['Rh'], dtype=torch.float32)     # (1, 3)\n",
    "body_pose     = torch.tensor(smpl_data['poses'], dtype=torch.float32)  # (1, 69)\n",
    "betas         = torch.tensor(smpl_data['shapes'], dtype=torch.float32) # (1, 10)\n",
    "transl        = torch.tensor(smpl_data['Th'], dtype=torch.float32)     # (1, 3)\n",
    "\n",
    "# Run SMPL\n",
    "output = model(\n",
    "    global_orient=global_orient,\n",
    "    body_pose=body_pose,\n",
    "    betas=betas,\n",
    "    transl=transl,\n",
    "    return_verts=True\n",
    ")\n",
    "\n",
    "verts = output.vertices  # (1, 6890, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01442ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 45, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f26c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import smplx\n",
    "import trimesh\n",
    "from pythreejs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fcf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e73fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ballet/smpl_joints/000000.json\", \"r\") as f:\n",
    "    keypoints = np.array(json.load(f)[0]['joints3d'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db9658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"smplx\"  # CHANGE THIS to your actual path\n",
    "model = smplx.create(model_path, model_type='smpl', gender='neutral', use_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dff6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef944bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(betas=torch.zeros(1, 10),\n",
    "               body_pose=torch.zeros(1, 69),\n",
    "               global_orient=torch.zeros(1, 3),\n",
    "               transl=torch.zeros(1, 3))\n",
    "vertices = output.vertices.detach().cpu().numpy().squeeze()\n",
    "faces = model.faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad63caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_geom = BufferGeometry(\n",
    "    attributes={\n",
    "        \"position\": BufferAttribute(vertices.astype(np.float32)),\n",
    "        \"index\": BufferAttribute(faces.astype(np.uint32).flatten())\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a8eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_material = MeshStandardMaterial(color='peachpuff', metalness=0.1, roughness=0.6, side='DoubleSide')\n",
    "smpl_mesh = Mesh(geometry=smpl_geom, material=smpl_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be15f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_spheres = []\n",
    "for joint in keypoints:\n",
    "    sphere_geom = SphereGeometry(radius=0.02)\n",
    "    sphere_material = MeshStandardMaterial(color='skyblue')\n",
    "    sphere = Mesh(geometry=sphere_geom, material=sphere_material, position=tuple(joint))\n",
    "    joint_spheres.append(sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ef663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0374228eba49ae810ed1ab9e37f319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(intensity=0.5, position=(3.0, 5.0, 1.0), quaterni…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scene = Scene(children=[smpl_mesh, *joint_spheres, AmbientLight(intensity=0.5)])\n",
    "\n",
    "# Add camera\n",
    "camera = PerspectiveCamera(position=[2, 0, 2], fov=50,\n",
    "                           children=[DirectionalLight(position=[3, 5, 1], intensity=0.5)])\n",
    "\n",
    "# Renderer\n",
    "renderer = Renderer(camera=camera, scene=scene, controls=[OrbitControls(controlling=camera)],\n",
    "                    width=800, height=600)\n",
    "\n",
    "display(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "862e3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ballet/smpl/000000.json\", \"r\") as f:\n",
    "    data = json.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bdd45f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ballet/smpl/000000.json\", \"r\") as f:\n",
    "    data = json.load(f)[0]\n",
    "pose = torch.tensor(data['poses'], dtype=torch.float32)\n",
    "betas = torch.tensor(data['shapes'], dtype=torch.float32)\n",
    "Rh = torch.tensor(data['Rh'], dtype=torch.float32)\n",
    "# Rh=torch.zeros((1,3))\n",
    "Th = torch.tensor(data['Th'], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rh = Rh * torch.tensor([1, math.pi, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "54201cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8230, -0.3613,  0.2800]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "66b22890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"smplx\"  # CHANGE this to your actual SMPL model path\n",
    "model = smplx.create(model_path, model_type='smpl', gender='neutral', use_pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "4a3ff6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(betas=betas, body_pose=pose, global_orient=Rh, transl=Th)\n",
    "vertices = output.vertices.detach().cpu().numpy().squeeze()\n",
    "faces = model.faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "511680ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints=output.joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1483f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "d1d092f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304a501c79b043c6bc43bbf0fca1cefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(fov=45.0, position=(5.0, 3.0, 1.5), projectionMatrix=(1.0, 0.0, 0.0, 0.0, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Convert to BufferGeometry\n",
    "geometry = BufferGeometry(\n",
    "    attributes={\n",
    "        'position': BufferAttribute(vertices.astype(np.float32)),\n",
    "        'index': BufferAttribute(faces.astype(np.uint32).flatten())\n",
    "    }\n",
    ")\n",
    "material = MeshStandardMaterial(\n",
    "    color='peachpuff',\n",
    "    roughness=0.6,\n",
    "    metalness=0.1,\n",
    "    side='DoubleSide',\n",
    "    transparent=True,\n",
    "    opacity=0.3  # Adjust to control transparency level\n",
    ")\n",
    "mesh = Mesh(geometry=geometry, material=material)\n",
    "# mesh.scale = [1, -1, 1]\n",
    "\n",
    "# Scene setup\n",
    "scene = Scene(children=[\n",
    "    mesh,\n",
    "    AmbientLight(intensity=0.5),\n",
    "    DirectionalLight(position=[3, 5, 1], intensity=0.6)\n",
    "])\n",
    "\n",
    "camera = PerspectiveCamera(position=[5, 3, 1.5], fov=45)\n",
    "controls = OrbitControls(controlling=camera)\n",
    "\n",
    "renderer = Renderer(camera=camera, scene=scene, controls=[controls],\n",
    "                    width=800, height=600)\n",
    "display(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "2c27c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smpl_to_threejs(coords):\n",
    "    coords = np.array(coords)\n",
    "    coords[..., 1] *= -1  # Flip Y\n",
    "    coords[..., 2] *= -1  # Flip Z ✅\n",
    "    # coords[..., 0] *= -1  # Flip Z\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "8f908df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints=output.joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b5682e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=smpl_to_threejs(joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54e11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "8a202a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a75e468f734822a33bf694e16bd818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(fov=45.0, position=(5.0, 3.0, 1.5), projectionMatrix=(1.0, 0.0, 0.0, 0.0, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with open(\"ballet/smpl_joints/000000.json\", \"r\") as f:\n",
    "#     joints = np.array(json.load(f)[0]['joints3d'])\n",
    "\n",
    "# Create joint spheres\n",
    "joint_spheres = []\n",
    "for joint in j[0]:\n",
    "    sphere_geom = SphereGeometry(radius=0.01)  # Adjust radius if needed\n",
    "    sphere_material = MeshStandardMaterial(color='skyblue')\n",
    "    sphere = Mesh(geometry=sphere_geom, material=sphere_material, position=tuple(joint))\n",
    "    joint_spheres.append(sphere)\n",
    "\n",
    "# Add everything to the scene again\n",
    "scene = Scene(children=[\n",
    "    mesh,  # your SMPL mesh\n",
    "    *joint_spheres,  # the spheres for joints\n",
    "    AmbientLight(intensity=0.5),\n",
    "    DirectionalLight(position=[3, 5, 1], intensity=0.6)\n",
    "])\n",
    "\n",
    "camera = PerspectiveCamera(position=[5, 3, 1.5], fov=45)\n",
    "controls = OrbitControls(controlling=camera)\n",
    "\n",
    "renderer = Renderer(camera=camera, scene=scene, controls=[controls],\n",
    "                    width=800, height=600)\n",
    "\n",
    "display(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6bb07f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45, 3)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(joints).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "dfcf7768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45, 3)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "804b2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from visualization.joints2bvh import Joint2BVHConvertor\n",
    "converter = Joint2BVHConvertor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "5b01ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\TP\\Desktop\\genAI\\BAMM\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[402], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_bvh\u001b[38;5;241m=\u001b[39m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoints\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mballet.bvh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\visualization\\joints2bvh.py:56\u001b[0m, in \u001b[0;36mJoint2BVHConvertor.convert\u001b[1;34m(self, positions, filename, iterations, foot_ik, fps)\u001b[0m\n\u001b[0;32m     53\u001b[0m new_anim\u001b[38;5;241m.\u001b[39mpositions[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m positions[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foot_ik:\n\u001b[1;32m---> 56\u001b[0m     positions \u001b[38;5;241m=\u001b[39m \u001b[43mremove_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfid_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfid_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterp_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mforce_on_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m ik_solver \u001b[38;5;241m=\u001b[39m BasicInverseKinematics(new_anim, positions, iterations\u001b[38;5;241m=\u001b[39miterations, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m new_anim \u001b[38;5;241m=\u001b[39m ik_solver()\n",
      "File \u001b[1;32mc:\\Users\\TP\\Desktop\\genAI\\BAMM\\visualization\\remove_fs.py:238\u001b[0m, in \u001b[0;36mremove_fs\u001b[1;34m(glb, foot_contact, fid_l, fid_r, interp_length, force_on_floor)\u001b[0m\n\u001b[0;32m    236\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m s \u001b[38;5;241m<\u001b[39m T:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m s \u001b[38;5;241m<\u001b[39m T \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mfixed\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m         s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m T:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "new_bvh=converter.convert(np.array(joints), \"ballet.bvh\", iterations=100, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "4fe96302",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[508], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my\u001b[49m,z\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e76b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def rotate_scene(points, axis='x', degrees=15):\n",
    "    rot = R.from_euler(axis, degrees, degrees=True).as_matrix()\n",
    "    return np.matmul(points, rot.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02ee8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smpl_to_threejs(coords):\n",
    "    coords = np.array(coords)\n",
    "\n",
    "    # Flip axes to convert SMPL to Three.js-friendly coordinate space\n",
    "    coords[..., 1] *= -1  # Flip Y\n",
    "    coords[..., 2] *= -1  # Flip Z\n",
    "\n",
    "    # Apply tilt correction (rotate 15 degrees arou nd X-axis to straighten body)\n",
    "    # coords = rotate_scene(coords, axis='x', degrees=15)\n",
    "\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23191514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Collected joints shape: (3000, 45, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_15352\\1919913801.py:52: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  Rh_all = np.array(Rh_all)\n",
      "C:\\Users\\TP\\AppData\\Local\\Temp\\ipykernel_15352\\1919913801.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Rh_all = np.array(Rh_all)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import smplx\n",
    "\n",
    "# SMPL model setup\n",
    "model_path = \"smplx\"  # CHANGE this to your actual SMPL model path\n",
    "model = smplx.create(model_path, model_type='smpl', gender='neutral', use_pca=False)\n",
    "\n",
    "joints_all = []\n",
    "\n",
    "# Path to your directory\n",
    "json_dir = \"jazz/smpl\"\n",
    "Rh_all=[]\n",
    "poses_all=[]\n",
    "\n",
    "# Loop through all .json files in the directory\n",
    "for file_name in sorted(os.listdir(json_dir)[:3000]):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(json_dir, file_name), \"r\") as f:\n",
    "            data = json.load(f)[0]\n",
    "\n",
    "        # Load pose params\n",
    "        pose = torch.tensor(data['poses'], dtype=torch.float32)\n",
    "        betas = torch.tensor(data['shapes'], dtype=torch.float32)\n",
    "        Rh = torch.tensor(data['Rh'],dtype=torch.float32)\n",
    "\n",
    "        Th = torch.tensor(data['Th'], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        output = model(\n",
    "            betas=betas,\n",
    "            body_pose=pose,\n",
    "            global_orient=Rh,\n",
    "            transl=Th\n",
    "        )\n",
    "\n",
    "        # Rh=Rh*torch.tensor([1,-1,-1])\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(betas=betas, body_pose=pose, global_orient=Rh, transl=Th)\n",
    "        joints = output.joints.detach().cpu().numpy().squeeze()\n",
    "        joints=smpl_to_threejs(joints)\n",
    "        Rh_all.append(Rh)\n",
    "        poses_all.append(torch.cat([Rh[0], pose[0]], dim=0))\n",
    "        joints_all.append(joints)\n",
    "\n",
    "# Convert to a NumPy array if needed\n",
    "joints_all = np.array(joints_all)  # shape: (num_frames, num_joints, 3)\n",
    "Rh_all = np.array(Rh_all)\n",
    "print(\"Collected joints shape:\", joints_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6495177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of 72D pose vectors to tensor\n",
    "poses_all_tensor = torch.stack(poses_all) \n",
    "poses_all_tensor = poses_all_tensor.view(-1, 24, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0672bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_np = poses_all_tensor.detach().cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a104c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from visualization.joints2bvh import Joint2BVHConvertor\n",
    "converter = Joint2BVHConvertor()\n",
    "new_bvh=converter.convert(positions_np, \"jazz.bvh\", iterations=100,fps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e57e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Tanmay Files\\\\RA\\\\threejs\\\\jazz.bvh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src = \"jazz.bvh\"\n",
    "path = r\"D:\\Tanmay Files\\RA\\threejs\"\n",
    "\n",
    "shutil.copy(src, path)  # Copies the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ad898dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Tanmay Files\\\\RA\\\\threejs\\\\ballet1.bvh'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src = \"ballet1.bvh\"\n",
    "path = r\"D:\\Tanmay Files\\RA\\threejs\"\n",
    "\n",
    "shutil.copy(src, path)  # Copies the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d86924db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Tanmay Files\\\\RA\\\\threejs\\\\ballet2.bvh'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src = \"ballet2.bvh\"\n",
    "path = r\"D:\\Tanmay Files\\RA\\threejs\"\n",
    "\n",
    "shutil.copy(src, path)  # Copies the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56438527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94f800d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import smplx\n",
    "\n",
    "# SMPL model setup\n",
    "model_path = \"smplx\"  # CHANGE this to your actual SMPL model path\n",
    "model = smplx.create(model_path, model_type='smpl', gender='neutral', use_pca=False)\n",
    "\n",
    "joints_all = []\n",
    "\n",
    "# Path to your directory\n",
    "json_dir = \"graham/smpl\"\n",
    "all_frames_rotations = []\n",
    "all_th=[]\n",
    "# Loop through all .json files in the directory\n",
    "for file_name in sorted(os.listdir(json_dir)):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(json_dir, file_name), \"r\") as f:\n",
    "            data = json.load(f)[0]\n",
    "\n",
    "        # Load pose params\n",
    "        pose = torch.tensor(data['poses'], dtype=torch.float32)\n",
    "        betas = torch.tensor(data['shapes'], dtype=torch.float32)\n",
    "        Rh = torch.tensor(data['Rh'],dtype=torch.float32)\n",
    "\n",
    "        Th = torch.tensor(data['Th'], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Rh=Rh*torch.tensor([1,-1,-1])\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        full_pose = torch.cat([Rh, pose], dim=1)  # (1, 72)\n",
    "\n",
    "    # Convert axis-angle to rotation matrices (24 joints × 3D)\n",
    "        rotvecs = full_pose.view(-1, 3).numpy()  # (24, 3)\n",
    "        rot_mats = R.from_rotvec(rotvecs).as_matrix()  # (24, 3, 3)\n",
    "\n",
    "        all_frames_rotations.append(rot_mats.tolist())  # Save as JSON-serializable\n",
    "        all_th.append(Th.tolist())\n",
    "\n",
    "# Save all rotation matrices for all frames\n",
    "with open(\"all_rotations.json\", \"w\") as f:\n",
    "    json.dump(all_frames_rotations, f)\n",
    "\n",
    "with open(\"all_th.json\", \"w\") as f:\n",
    "    json.dump(all_th, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe972ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8372, 1, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c382380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Tanmay Files\\\\RA\\\\threejs\\\\graham\\\\all_rotations.json'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src_path=\"all_rotations.json\"\n",
    "path = r\"D:\\Tanmay Files\\RA\\threejs\\graham\"\n",
    "shutil.copy(src=src_path,dst=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b98b1779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Tanmay Files\\\\RA\\\\threejs\\\\graham\\\\all_th.json'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src_path=\"all_th.json\"\n",
    "path = r\"D:\\Tanmay Files\\RA\\threejs\\graham\"\n",
    "shutil.copy(src=src_path,dst=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150064b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
