{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epinyoan/miniconda3/envs/momask/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from models.mask_transformer.transformer import MaskTransformer\n",
    "from models.mask_transformer.transformer_trainer import MaskTransformerTrainer\n",
    "from models.vq.model import RVQVAE\n",
    "\n",
    "from options.train_option import TrainT2MOptions\n",
    "\n",
    "from utils.plot_script import plot_3d_motion\n",
    "from utils.motion_process import recover_from_ric\n",
    "from utils.get_opt import get_opt\n",
    "from utils.fixseed import fixseed\n",
    "from utils.paramUtil import t2m_kinematic_chain, kit_kinematic_chain\n",
    "\n",
    "from data.t2m_dataset import Text2MotionDataset\n",
    "from motion_loaders.dataset_motion_loader import get_dataset_motion_loader\n",
    "from models.t2m_eval_wrapper import EvaluatorModelWrapper\n",
    "from exit.utils import visualize_2motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batch_size: 64\n",
      "checkpoints_dir: ./log/t2m\n",
      "cond_drop_prob: 0.1\n",
      "dataset_name: t2m\n",
      "dropout: 0.2\n",
      "eval_every_e: 10\n",
      "ff_size: 1024\n",
      "force_mask: False\n",
      "gamma: 0.1\n",
      "gpu_id: -1\n",
      "gumbel_sample: False\n",
      "is_continue: False\n",
      "is_train: True\n",
      "latent_dim: 384\n",
      "log_every: 50\n",
      "lr: 0.0002\n",
      "max_epoch: 500\n",
      "max_motion_length: 196\n",
      "milestones: [50000]\n",
      "n_heads: 6\n",
      "n_layers: 8\n",
      "name: 2024-05-22-20-06-48_t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns\n",
      "save_latest: 500\n",
      "seed: 3407\n",
      "share_weight: False\n",
      "trans: official\n",
      "unit_length: 4\n",
      "vq_name: rvq_nq1_dc512_nc512\n",
      "warm_up_iter: 2000\n",
      "-------------- End ----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-05-22-20-06-48_t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from options.train_option import TrainT2MOptions\n",
    "parser = TrainT2MOptions()\n",
    "opt = parser.parse(is_mock=True)\n",
    "opt.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.data_root = './dataset/HumanML3D'\n",
    "opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')\n",
    "opt.joints_num = 22\n",
    "opt.max_motion_len = 55\n",
    "dim_pose = 263\n",
    "radius = 4\n",
    "fps = 20\n",
    "kinematic_chain = t2m_kinematic_chain\n",
    "dataset_opt_path = './checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/epinyoan/git/momask-codes/checkpoints/t2m/rvq_nq6_dc512_nc512_noshare_qdp0.2/opt.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VQ Model rvq_nq6_dc512_nc512_noshare_qdp0.2\n"
     ]
    }
   ],
   "source": [
    "opt.vq_name = 'rvq_nq6_dc512_nc512_noshare_qdp0.2'\n",
    "vq_opt = get_opt(f'/home/epinyoan/git/momask-codes/checkpoints/t2m/{opt.vq_name}/opt.txt', 'cuda')\n",
    "vq_model = RVQVAE(vq_opt,\n",
    "                dim_pose,\n",
    "                vq_opt.nb_code,\n",
    "                vq_opt.code_dim,\n",
    "                vq_opt.output_emb_width,\n",
    "                vq_opt.down_t,\n",
    "                vq_opt.stride_t,\n",
    "                vq_opt.width,\n",
    "                vq_opt.depth,\n",
    "                vq_opt.dilation_growth_rate,\n",
    "                vq_opt.vq_act,\n",
    "                vq_opt.vq_norm)\n",
    "ckpt = torch.load(pjoin(vq_opt.checkpoints_dir, vq_opt.dataset_name, vq_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                        map_location='cpu')\n",
    "model_key = 'vq_model' if 'vq_model' in ckpt else 'net'\n",
    "vq_model.load_state_dict(ckpt[model_key])\n",
    "print(f'Loading VQ Model {opt.vq_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n",
      "Loading CLIP...\n"
     ]
    }
   ],
   "source": [
    "opt.log_dir = pjoin('./log/t2m/', opt.dataset_name, opt.name)\n",
    "opt.device = 'cuda' #torch.device(\"cpu\" if opt.gpu_id == -1 else \"cuda:\" + str(opt.gpu_id))\n",
    "opt.text_dir = pjoin(opt.data_root, 'texts')\n",
    "opt.num_tokens = vq_opt.nb_code\n",
    "clip_version = 'ViT-B/32'\n",
    "# opt.trans = 't2mgpt'\n",
    "opt.trans = 'official'\n",
    "\n",
    "t2m_transformer = MaskTransformer(code_dim=vq_opt.code_dim,\n",
    "                                      cond_mode='text',\n",
    "                                      latent_dim=opt.latent_dim,\n",
    "                                      ff_size=opt.ff_size,\n",
    "                                      num_layers=opt.n_layers,\n",
    "                                      num_heads=opt.n_heads,\n",
    "                                      dropout=opt.dropout,\n",
    "                                      clip_dim=512,\n",
    "                                      cond_drop_prob=opt.cond_drop_prob,\n",
    "                                      clip_version=clip_version,\n",
    "                                      opt=opt)\n",
    "# t2m_transformer.load_and_freeze_token_emb(vq_model.quantizer_upper.codebooks[0], vq_model.quantizer_lower.codebooks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/epinyoan/git/momask-codes/log/t2m/t2m/2024-02-14-14-27-29_8_GPT_officialTrans_2iterPrdictEnd/model/net_best_fid.tar'\n",
    "checkpoint = torch.load(model_dir, map_location='cuda')\n",
    "missing_keys, unexpected_keys = t2m_transformer.load_state_dict(checkpoint['t2m_transformer'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.load(pjoin('./checkpoints/', opt.dataset_name, opt.vq_name, 'meta', 'mean.npy'))\n",
    "std = np.load(pjoin('./checkpoints/', opt.dataset_name, opt.vq_name, 'meta', 'std.npy'))\n",
    "\n",
    "train_split_file = pjoin(opt.data_root, 'train.txt')\n",
    "val_split_file = pjoin(opt.data_root, 'val.txt')\n",
    "\n",
    "train_dataset = Text2MotionDataset(opt, mean, std, train_split_file)\n",
    "val_dataset = Text2MotionDataset(opt, mean, std, val_split_file)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=opt.batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "eval_val_loader, _ = get_dataset_motion_loader(dataset_opt_path, 32, 'test', device=opt.device)\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2m_transformer.to(opt.device)\n",
    "vq_model.to(opt.device)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/epinyoan/git/momask-codes/checkpoints/t2m/tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw/opt.txt\n",
      "latent_dim: 384, ff_size: 1024, nlayers: 8, nheads: 6, dropout: 0.2\n",
      "Loading CLIP...\n",
      "Loading Residual Transformer tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw from epoch 440!\n"
     ]
    }
   ],
   "source": [
    "from models.mask_transformer.transformer import ResidualTransformer\n",
    "def load_res_model(res_opt):\n",
    "    res_opt.num_quantizers = vq_opt.num_quantizers\n",
    "    res_opt.num_tokens = vq_opt.nb_code\n",
    "    res_transformer = ResidualTransformer(code_dim=vq_opt.code_dim,\n",
    "                                            cond_mode='text',\n",
    "                                            latent_dim=res_opt.latent_dim,\n",
    "                                            ff_size=res_opt.ff_size,\n",
    "                                            num_layers=res_opt.n_layers,\n",
    "                                            num_heads=res_opt.n_heads,\n",
    "                                            dropout=res_opt.dropout,\n",
    "                                            clip_dim=512,\n",
    "                                            shared_codebook=vq_opt.shared_codebook,\n",
    "                                            cond_drop_prob=res_opt.cond_drop_prob,\n",
    "                                            # codebook=vq_model.quantizer.codebooks[0] if opt.fix_token_emb else None,\n",
    "                                            share_weight=res_opt.share_weight,\n",
    "                                            clip_version=clip_version,\n",
    "                                            opt=res_opt)\n",
    "\n",
    "    ckpt = torch.load(pjoin(res_opt.checkpoints_dir, res_opt.dataset_name, res_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                      map_location=opt.device)\n",
    "    missing_keys, unexpected_keys = res_transformer.load_state_dict(ckpt['res_transformer'], strict=False)\n",
    "    assert len(unexpected_keys) == 0\n",
    "    assert all([k.startswith('clip_model.') for k in missing_keys])\n",
    "    print(f'Loading Residual Transformer {res_opt.name} from epoch {ckpt[\"ep\"]}!')\n",
    "    return res_transformer\n",
    "res_opt = get_opt('/home/epinyoan/git/momask-codes/checkpoints/t2m/tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw/opt.txt', device=opt.device)\n",
    "res_model = load_res_model(res_opt).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2m_transformer.eval()\n",
    "vq_model.eval()\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Motion (from dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__break__ = False\n",
    "while __break__ is False:\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token in eval_val_loader:\n",
    "    # for clip_text, pose, m_length in train_loader:\n",
    "        for i, text in enumerate(clip_text):\n",
    "            # if True: #'cartwheel' in text:\n",
    "            # if 'a person walks forward then turns completely around and does a cartwheel.' in text:\n",
    "            # if 'a person who is standing moves his body and shoulders slightly to the left and returns to his original position.' in text:\n",
    "            # if 'person carefully kneels down. first with right leg, then with the left one.' in text:\n",
    "            # if 'a person walks forward then turns completely around and does a cartwheel.' in text:\n",
    "            # if 'a person with their arms out from their sides, walks to the left trying to balance.' in text:\n",
    "            # if 'a person walks forward, then is pushed to their left and then returns to walking in the line they were.' in text:\n",
    "            # if 'the person crouches and walks forward.' in text:\n",
    "            if len(text) < 100:\n",
    "                __break__ = True\n",
    "                print(i, m_length[i], text)\n",
    "                break\n",
    "        if __break__:\n",
    "            break\n",
    "m_length = m_length.cuda()\n",
    "base_pose = pose.cuda()\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 124 a person takes two steps forward leading with their right foot rotates on their left foot 180 degrees and takes two steps back.\n",
      "1 196 standing on their right foot, a person flexes their knee forward and back and side to side, toward the right first and then the left.\n",
      "2 188 the person slides to their right 3 times, slides to their left 4 times, and slides to their left 2 times, ending at their original position.\n",
      "3 196 person squats down to their knees, and then begins to crawl around in random directions. seems to be playful\n",
      "4 144 a person walks diagonally to the left, bends over and picks up a few items, then continues walking\n",
      "5 196 a man sits down on the ground cross-legged and throws an object with his right hand.\n",
      "6 84 a person who is standing with his hands at his sides quickly runs forward and stops.\n",
      "7 64 the person steps back on their left leg before pitching a baseball forward.\n",
      "8 172 person was standing and straddled a chair and sat down.\n",
      "9 196 a man paces back and forth along the same line.\n",
      "10 192 a person standing in the same spot while rocking sideways.\n",
      "11 196 a figure repeatedly turns towards the right hand side\n",
      "12 88 a person is repeatedly raising and lowering their forearms.\n",
      "13 84 moving hands from side to side above head.\n",
      "14 64 a person walks forward quickly and then stops\n",
      "15 128 the man is kicking with his right hand\n",
      "16 192 person is doing an excercise with hands forward.\n",
      "17 60 character lifts both hands up to face twice.\n",
      "18 92 a person takes a small hop forward.\n",
      "19 124 sitting down on the ground then standing.\n",
      "20 104 a person strolls forward towards the right\n",
      "21 40 person walks quickly down a short incline\n",
      "22 40 a person is leaning to the right.\n",
      "23 144 a person walks in a clockwise circle.\n",
      "24 40 a man jumps from left to right.\n",
      "25 192 walking forward then to the side.\n",
      "26 76 a person doing a casual walk\n",
      "27 132 a person walks forward then stops.\n",
      "28 136 the person threw some thing forward.\n",
      "29 48 person moving arm in up motion\n",
      "30 156 this person is cleaning a table.\n",
      "31 152 a person stepping over something\n"
     ]
    }
   ],
   "source": [
    "word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token = next(iter(eval_val_loader))\n",
    "# clip_text, pose, m_length = next(iter(train_loader))\n",
    "m_length = m_length.cuda()\n",
    "base_pose = pose.cuda()\n",
    "for i in range(len(clip_text)):\n",
    "    # if len(clip_text[i]) < 100 and m_length[i]>=180: # \n",
    "    print(i, m_length[i].cpu().numpy(), clip_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 196 a person throws a shotput.\n",
    "# the person crouches and walks forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a person stands up from a sit down position, then sits back down.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=13\n",
    "clip_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 3\n",
    "cond_scale = 4\n",
    "mids = t2m_transformer.generate(clip_text, m_length, time_steps, cond_scale, temperature=1) # , temp=code_idx[..., 0]\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids = res_model.generate(mids, clip_text, pred_len, temperature=1, cond_scale=5)\n",
    "# mids =  mids.unsqueeze_(-1)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]\n",
    "\n",
    "k = i\n",
    "x = pred_motions[k].detach().cpu().numpy()\n",
    "l = pred_len[k]*4 # m_length[k] #\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l, base_pose[k].cpu().numpy())\n",
    "pred_len[k]*4, clip_text[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Motion (from Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 100\n",
    "# a person appears to be ding jump rope until they fail\n",
    "# walking in a circle with hands up.\n",
    "# 'a person starts walking then starts leaping counterclockwise' # momask fails at 156\n",
    "# a person tip toeing in an l shape figure to the right. # momask not bad\n",
    "\n",
    "# [Momask Not fail]\n",
    "# A man starts off with both arms extended out by his sides. He then brings his arms down to his body and claps his hands together. After this, he walks down and to the right where he proceeds to sit on a seat.\n",
    "# [Cannot replicate]\n",
    "# A person ducks down to walk underneath a fence in front of them. Walk around something, then turn left again to climb up a something\n",
    "# [Momask Not fail]\n",
    "# the person slides to their right, slides to their left, and back forth, trying to move away  from something in a game\n",
    "clip_text1 = ['a person walks in a zig zag motion then comes to a halt'] * repeat\n",
    "m_length1 = torch.tensor([1]*repeat).cuda()\n",
    "mids = t2m_transformer.generate(clip_text1, m_length1, timesteps=3, cond_scale=4, temperature=1) # , temp=code_idx[..., 0]\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids = res_model.generate(mids, clip_text1, pred_len, temperature=1, cond_scale=5)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pred_len)):\n",
    "#     # if pred_len[i] == 39:\n",
    "#     print(i, pred_len[i].cpu().detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x = pred_motions[i].detach().cpu().numpy()\n",
    "l = pred_len[i]*4 # m_length[k] #\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)\n",
    "pred_len[i]*4, clip_text1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 4\n",
    "# motion = pred_motions[i][: (pred_len[i]*4)].cpu().detach().numpy()\n",
    "np.save('./BAMM_a_person_starts_walking_then_starts_leaping_counterclockwise._156.npy.npy', pred_motions.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crouch_walk_together_156.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3544073/2900277679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'crouch_walk_together_156.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/momask/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'crouch_walk_together_156.npy'"
     ]
    }
   ],
   "source": [
    "motion = np.load('crouch_walk_together_156.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'motion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3544073/2606599827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvisualize_2motions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't2m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'motion' is not defined"
     ]
    }
   ],
   "source": [
    "x = motion\n",
    "l = motion.shape[0]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(49.) a man walks in a circle for 10 seconds to stretch his legs after sitting for an extended period of time.\n",
      "7 tensor(49.) a person walks forward carefully placing one foot directly in front of the other foot.\n",
      "13 tensor(49.) a person walks forward, turns, then sits, then stands and walks back\n",
      "15 tensor(49.) a person aggressively grabbing holding something before drinking it.\n",
      "19 tensor(49.) a person is leaning forward and reaching for something.\n",
      "23 tensor(49.) the person has washing his right arm.\n",
      "28 tensor(49.) person stands there and looks around.\n"
     ]
    }
   ],
   "source": [
    "__break__ = False\n",
    "while __break__ is False:\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token in eval_val_loader:\n",
    "        for i, text in enumerate(clip_text):\n",
    "            # if 'a person, whose hands is swaying front to back, is walking in a counter-clockwise circle while bringing their knees up as they walk.' in text:\n",
    "            #     __break__ = True\n",
    "            if m_length[i] >= 196:\n",
    "                print(i, m_length[i]/4, text)\n",
    "                __break__ = True\n",
    "                # break\n",
    "        if __break__:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = ['the person is stretching arms out.']\n",
    "k = 1\n",
    "txt = [clip_text[k]]\n",
    "input_len = torch.tensor([49]).cuda()\n",
    "mids = t2m_transformer.generate(txt, input_len, timesteps = 3, cond_scale = 4, temperature=1) # , temp=code_idx[..., 0]\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids = res_model.generate(mids, txt, pred_len, temperature=1, cond_scale=5)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]\n",
    "\n",
    "x = pred_motions[0].detach().cpu().numpy()\n",
    "l = pred_len[0]*4 #pred_len[k]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l, pose[k].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit (Generated Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 3\n",
    "cond_scale = 4\n",
    "# base_text = ['a figure walks forward']\n",
    "base_text = ['a person walks forward.']\n",
    "cond_tokens = t2m_transformer.pad_id * torch.ones((1, 50), dtype=torch.long).cuda()\n",
    "cond_tokens = cond_tokens.scatter_(-1, torch.tensor([[49]]).cuda(), t2m_transformer.end_id).long() # add end token to tell model where to stop generating\n",
    "index_motion = t2m_transformer.edit2(base_text, cond_tokens)\n",
    "# index_motion = t2m_transformer.generate(base_text, m_length, time_steps, cond_scale, temperature=1)\n",
    "index_motion, pred_len = t2m_transformer.pad_when_end(index_motion)\n",
    "index_motion = res_model.generate(index_motion, base_text, pred_len, temperature=1, cond_scale=5)\n",
    "mids =  mids.unsqueeze_(-1)\n",
    "pred_motions = vq_model.forward_decoder(index_motion)[:, :196]\n",
    "\n",
    "k = 0\n",
    "x = pred_motions[k].detach().cpu().numpy()\n",
    "l = pred_len[k]*4 #pred_len[k]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = ['a person jump in place.']\n",
    "left = int(pred_len[k]/4)\n",
    "right = int(pred_len[k]-left-4)\n",
    "cond_tokens = index_motion.clone()\n",
    "cond_tokens[k, :left] = t2m_transformer.pad_id\n",
    "cond_tokens[k, right:] = t2m_transformer.pad_id\n",
    "cond_tokens = cond_tokens[..., 0]\n",
    "cond_tokens = cond_tokens.scatter_(-1, pred_len[..., None], t2m_transformer.end_id).long() # add end token to tell model where to stop generating\n",
    "mids = t2m_transformer.edit2(gen_text, cond_tokens)\n",
    "mids[k, pred_len[k]:] = t2m_transformer.pad_id\n",
    "mids = res_model.generate(mids, gen_text, pred_len, temperature=1, cond_scale=5)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]\n",
    "\n",
    "x = pred_motions[k].detach().cpu().numpy()\n",
    "l = pred_len[k] * 4\n",
    "caption = text[k]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('outpainting.npy', x)\n",
    "# x = np.load('./outpainting.npy')\n",
    "# l = x.shape[0]\n",
    "# caption = text[k]\n",
    "# visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit (Test Set Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(196) a person walks up to shake with their left hand, turns slightly left to shake again, and turns left again to shake for a final time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__break__ = False\n",
    "while __break__ is False:\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token in eval_val_loader:\n",
    "        for i, text in enumerate(clip_text):\n",
    "            # if True: #'cartwheel' in text:\n",
    "            # if 'a person walks forward then turns completely around and does a cartwheel.' in text:\n",
    "            # if 'a person who is standing moves his body and shoulders slightly to the left and returns to his original position.' in text:\n",
    "            # if 'person carefully kneels down. first with right leg, then with the left one.' in text:\n",
    "            # if 'a person walks forward then turns completely around and does a cartwheel.' in text:\n",
    "            # if 'a person with their arms out from their sides, walks to the left trying to balance.' in text:\n",
    "            # if 'a person walks forward, then is pushed to their left and then returns to walking in the line they were.' in text:\n",
    "            if 'walk' in text:\n",
    "                __break__ = True\n",
    "                print(i, m_length[i], text)\n",
    "                break\n",
    "        if __break__:\n",
    "            break\n",
    "m_length = m_length[i:i+1].cuda()\n",
    "base_pose = pose[i:i+1].cuda()\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = ['A person sits down.']\n",
    "bs, seq = pose.shape[:2]\n",
    "\n",
    "# cond prefix tokens\n",
    "# half_l = int(m_length/2) # condition by first half\n",
    "# token_length = (m_length/4).long() # downsampling 4 times\n",
    "# cond_tokens = t2m_transformer.pad_id * torch.ones((1, 50), dtype=torch.long).cuda()\n",
    "\n",
    "# index_motion, _ = vq_model.encode(base_pose[:, :half_l].cuda())\n",
    "# index_motion = index_motion[..., 0]\n",
    "\n",
    "# cond_tokens[:, :index_motion.shape[1]] = index_motion\n",
    "# cond_tokens = cond_tokens.scatter_(-1, token_length[..., None], t2m_transformer.end_id).long() # add end token to tell model where to stop generating\n",
    "\n",
    "\n",
    "# cond outpainting\n",
    "tokens = t2m_transformer.pad_id * torch.ones((bs, 50), dtype=torch.long).cuda()\n",
    "m_token_length = torch.ceil((m_length)/4).int().cpu().numpy()\n",
    "m_token_length_init = (m_token_length * .25).astype(int)\n",
    "m_length_init = (m_length * .25).int()\n",
    "\n",
    "k=0\n",
    "l = m_length_init[k]\n",
    "l_token = m_token_length_init[k]\n",
    "\n",
    "index_motion, _ = vq_model.encode(pose[k:k+1, l:m_length[k]-l].cuda())\n",
    "index_motion = index_motion[..., 0]\n",
    "tokens[k, l_token: l_token+index_motion.shape[1]] = index_motion[0]\n",
    "cond_tokens = tokens[k:k+1]\n",
    "\n",
    "\n",
    "mids = t2m_transformer.edit2(gen_text, cond_tokens)\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids = res_model.generate(mids, gen_text, pred_len, temperature=1, cond_scale=5)\n",
    "# mids =  mids.unsqueeze_(-1)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "x = pred_motions[k].detach().cpu().numpy()\n",
    "l = m_length[k]\n",
    "caption = text[k]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l, base_pose[i].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 3\n",
    "cond_scale = 4\n",
    "clip_text = ['a person is doing a salsa dance moving their legs and arms.',\n",
    "             'a figure sits on the chair.',\n",
    "            #  'a person squatting raises both their arms above their head.',\n",
    "             'a man jump forward.',\n",
    "             'a person punches as if they are boxing.',\n",
    "             'A woman crawl around.',\n",
    "             'A person is running forward in a long line.'\n",
    "             ]\n",
    "m_length = torch.tensor([0]).cuda()\n",
    "mids = t2m_transformer.generate(clip_text, m_length, time_steps, cond_scale, temperature=1) # , temp=code_idx[..., 0]\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "mids = res_model.generate(mids, clip_text, pred_len, temperature=1, cond_scale=5)\n",
    "# mids =  mids.unsqueeze_(-1)\n",
    "pred_motions = vq_model.forward_decoder(mids)[:, :196]\n",
    "\n",
    "k = 0\n",
    "x = pred_motions[k].detach().cpu().numpy()\n",
    "l = pred_len[k]*4 #pred_len[k]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transition_token = 2\n",
    "b = mids.shape[0]\n",
    "\n",
    "half_token_length = (pred_len/2).int()\n",
    "idx_full_len = half_token_length >= 24\n",
    "half_token_length[idx_full_len] = half_token_length[idx_full_len] - 1\n",
    "tokens = -1 * torch.ones((b-1, 50), dtype=torch.long).cuda()\n",
    "transition_train_length = []\n",
    "for i in range(b-1):\n",
    "    i_index_motion = mids[i ,: , 0]\n",
    "    i1_index_motion = mids[i+1 ,: , 0]\n",
    "\n",
    "    left_end = half_token_length[i]\n",
    "    right_start = left_end + num_transition_token\n",
    "    end = right_start + half_token_length[i+1]\n",
    "\n",
    "    tokens[i, :left_end] = i_index_motion[pred_len[i]-left_end: pred_len[i]]\n",
    "    tokens[i, left_end:right_start] = t2m_transformer.pad_id\n",
    "    tokens[i, right_start:end] = i1_index_motion[:half_token_length[i+1]]\n",
    "    transition_train_length.append(end)\n",
    "transition_train_length = torch.tensor(transition_train_length).to(mids.device).long()\n",
    "tokens = tokens.scatter_(-1, transition_train_length[..., None], t2m_transformer.end_id).long() # add end token to tell model where to stop generating\n",
    "inpainting_mask = tokens == t2m_transformer.pad_id\n",
    "tokens[tokens == -1] = t2m_transformer.pad_id\n",
    "inpaint_index = t2m_transformer.edit2(None, tokens)\n",
    "\n",
    "# residual \n",
    "all_tokens = []\n",
    "for i in range(b-1):\n",
    "    all_tokens.append(mids[i, :pred_len[i]])\n",
    "\n",
    "    inpaint_all = inpaint_index[i, inpainting_mask[i]].unsqueeze(-1)\n",
    "    inpaint_all = torch.nn.functional.pad(inpaint_all,  (0, 5),  value = -1)\n",
    "    all_tokens.append(inpaint_all)\n",
    "all_tokens.append(mids[-1, :pred_len[-1]])\n",
    "all_tokens = torch.cat(all_tokens).unsqueeze(0)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motions = vq_model.forward_decoder(all_tokens)\n",
    "k = 0\n",
    "x = pred_motions[k].detach().cpu().numpy()[:1000]\n",
    "l = pred_motions.shape[1]\n",
    "visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, 't2m', l, save_path='./longrange.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('longrange.npy', pred_motions[0].detach().cpu().numpy())\n",
    "# clip_text, pred_len*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "For testing load the correct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [01:45<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Ep 0 :, FID. 0.0799, Diversity Real. 9.5846, Diversity. 9.9658, R_precision_real. [0.51551724 0.70603448 0.79892241], R_precision. [0.50581897 0.68771552 0.78599138], matching_score_real. 2.978284403373455, matching_score_pred. 3.0599816914262443\n",
      "./Test/FID 0.0799292176316726 0\n",
      "./Test/Diversity 9.965841 0\n",
      "./Test/top1 0.5058189655172414 0\n",
      "./Test/top2 0.6877155172413794 0\n",
      "./Test/top3 0.7859913793103448 0\n",
      "./Test/matching_score 3.0599816914262443 0\n",
      "--> --> \t FID Improved from 100.00000 to 0.07993 !!!\n",
      "--> --> \t matching_score Improved from 100.00000 to 3.05998 !!!\n",
      "--> --> \t Diversity Improved from 100.00000 to 9.96584 !!!\n",
      "--> --> \t Top1 Improved from 0.0000 to 0.5058 !!!\n",
      "--> --> \t Top2 Improved from 0.0000 to 0.6877 !!!\n",
      "--> --> \t Top3 Improved from 0.0000 to 0.7860 !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_t2m import evaluation_mask_transformer\n",
    "class LoggerWriterMock:\n",
    "    def __init__(self):\n",
    "        self.info\n",
    "    def info(self, *args):\n",
    "        print(*args)\n",
    "    def add_scalar(self, *args):\n",
    "        print(*args)\n",
    "    def add_video(self, *args):\n",
    "        print(*args)\n",
    "logger = LoggerWriterMock()\n",
    "best_fid, best_div, best_top1, best_top2, best_top3, best_matching, writer = evaluation_mask_transformer(\n",
    "        'opt.save_root', eval_val_loader, t2m_transformer, vq_model, logger, 0,\n",
    "        best_fid=100, best_div=100,\n",
    "        best_top1=0, best_top2=0, best_top3=0,\n",
    "        best_matching=100, eval_wrapper=eval_wrapper,\n",
    "        plot_func=lambda x:x, save_ckpt=False, save_anim=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.len_predictor_modules import MotionLenEstimatorBiGRU\n",
    "from utils.word_vectorizer import WordVectorizer, POS_enumerator\n",
    "from torch.distributions.categorical import Categorical\n",
    "unit_length = 4\n",
    "dim_word = 300\n",
    "dim_pos_ohot = len(POS_enumerator)\n",
    "num_classes = 200 // unit_length\n",
    "estimator = MotionLenEstimatorBiGRU(dim_word, dim_pos_ohot, 512, num_classes)\n",
    "\n",
    "cp = '/home/epinyoan/git/momask-codes/checkpoints/t2m/length_est_bigru/model/latest.tar'\n",
    "checkpoints = torch.load(cp, map_location='cpu')\n",
    "estimator.load_state_dict(checkpoints['estimator'], strict=True)\n",
    "estimator.cuda()\n",
    "estimator.eval()\n",
    "softmax = torch.nn.Softmax(-1)\n",
    "\n",
    "def m_len_estimator(word_embeddings, pos_one_hots, sent_len):\n",
    "    pred_probs = estimator(word_embeddings.cuda().float(), pos_one_hots.cuda().float(), sent_len).detach()\n",
    "    pred_probs = softmax(pred_probs)\n",
    "    pred_tok_len = pred_probs.argsort(dim=-1, descending=True)[:, :5][..., 0]\n",
    "    # pred_tok_len = Categorical(pred_probs).sample()  # (b, seqlen)\n",
    "    # pred_tok_len = torch.multinomial(pred_probs, 1, replacement=True).squeeze()\n",
    "    return pred_tok_len # token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading dataset t2m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:01<00:00, 2655.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Ground Truth Dataset Loading Completed!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1232, device='cuda:0'), tensor(3416, device='cuda:0'))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "len_estimate_type = '' #'BAMM' \n",
    "eval_val_loader, _ = get_dataset_motion_loader(dataset_opt_path, 1024, 'test', device=opt.device, drop_last=False, shuffle=False)\n",
    "all_diff_len = None\n",
    "for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token in tqdm(eval_val_loader):\n",
    "    if len_estimate_type == 'BAMM':\n",
    "        mids = t2m_transformer.generate(clip_text, m_length.cuda(), timesteps = 3, cond_scale = 4, temperature=1) # , temp=code_idx[..., 0]\n",
    "        mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "        pred_len = pred_len * 4\n",
    "    else:\n",
    "        pred_len = m_len_estimator(word_embeddings, pos_one_hots, sent_len) * 4\n",
    "    diff_len = pred_len - m_length.cuda()\n",
    "    if all_diff_len is None:\n",
    "        all_diff_len = diff_len\n",
    "    else:\n",
    "        all_diff_len = torch.cat([all_diff_len, diff_len])\n",
    "all_diff_len\n",
    "(all_diff_len == 0).sum(), (all_diff_len != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tensor(530, device='cuda:0'), tensor(3566, device='cuda:0'))\n",
    "BAMM_all_diff_len = all_diff_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tensor(1753, device='cuda:0'), tensor(2343, device='cuda:0'))\n",
    "T2M_all_diff_len = all_diff_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epinyoan/miniconda3/envs/momask/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "/home/epinyoan/miniconda3/envs/momask/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:1449: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([T2M_all_diff_len.detach().cpu().numpy(), \n",
    "            BAMM_all_diff_len.detach().cpu().numpy()], \n",
    "            label=['text2length', 'BAMM'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2Length custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text = ['a person dances with one leg']\n",
    "tokens = ['sos/OTHER', 'a/DET', 'person/NOUN', 'dances/VERB', 'with/ADP', 'one/NUM', 'leg/NOUN', 'eos/OTHER']\n",
    "tokens = tokens + ['unk/OTHER'] * (22 - len(tokens))\n",
    "\n",
    "pos_one_hots = []\n",
    "word_embeddings = []\n",
    "for token in tokens:\n",
    "    word_emb, pos_oh = w_vectorizer[token]\n",
    "    pos_one_hots.append(pos_oh[None, :])\n",
    "    word_embeddings.append(word_emb[None, :])\n",
    "pos_one_hots = np.concatenate(pos_one_hots, axis=0)\n",
    "pos_one_hots = torch.from_numpy(pos_one_hots[np.newaxis, ...])\n",
    "word_embeddings = np.concatenate(word_embeddings, axis=0)\n",
    "word_embeddings = torch.from_numpy(word_embeddings[np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = estimator(word_embeddings.cuda().float(), pos_one_hots.cuda().float(), torch.tensor([len(clip_text)])).detach()\n",
    "prob = softmax(prob)\n",
    "prob = prob.detach().cpu().numpy()\n",
    "\n",
    "plt.bar(np.arange(prob.shape[1]), prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAMM len Single Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading dataset t2m ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:01<00:00, 2366.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Ground Truth Dataset Loading Completed!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_val_loader, _ = get_dataset_motion_loader(dataset_opt_path, 32, 'test', device=opt.device, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(42.) a person takes a step forward, turns around and walks back behind them, then turns around to face their front.\n",
      "1 tensor(49.) a person moves forward then left while making the motion of shaking the hands with the right arm, both arms and then the right arm again.\n",
      "2 tensor(24.) a man walks forward turns to the right and then walks back in the direct he came.\n",
      "3 tensor(16.) a person steps back with his left leg, dragging his right leg on the ground.\n",
      "4 tensor(23.) a person sways to swing their right foot followed by their left foot.\n",
      "5 tensor(29.) a person with their knees on the ground and their hands, gets up.\n",
      "6 tensor(12.) a person drives forward to the right and then curves to the left.\n",
      "7 tensor(35.) a man walks forward before stumbling backwards and the continues walking forward.\n",
      "8 tensor(48.) man man jumps up throws a ball high, then throws another lowe.\n",
      "9 tensor(42.) a person raises their arm and takes a small step forward\n",
      "10 tensor(48.) the person is walking forward and turn around like a monster\n",
      "11 tensor(25.) stick figure bends over picks somthing up then starts walking forward\n",
      "12 tensor(38.) a person sits with their legs crossed and then stands up.\n",
      "13 tensor(9.) a person steps backwards with their right foot and stretches.\n",
      "14 tensor(26.) a person casually takes a few steps forward before stopping.\n",
      "15 tensor(22.) a person jumps straight up with both arms down.\n",
      "16 tensor(38.) the person is sitting down i think scratching head.\n",
      "17 tensor(39.) a person slowly walks forward and then turns around.\n",
      "18 tensor(49.) a person walks forward in a normal fashion\n",
      "19 tensor(48.) the person is balancing on his left foot.\n",
      "20 tensor(16.) a person is walking in a straight line.\n",
      "21 tensor(41.) a man sits down, then gets back up.\n",
      "22 tensor(22.) the body wake gracefully with arms out straight.\n",
      "23 tensor(48.) a person walks in a counter clockwise circle.\n",
      "24 tensor(12.) a person stands with a wide defensive stance.\n",
      "25 tensor(37.) a person walks while dragging his feet.\n",
      "26 tensor(40.) a person steps carefully while walking forwards.\n",
      "27 tensor(34.) person is doing a ballet dance\n",
      "28 tensor(32.) person is kicking an athletic ball\n",
      "29 tensor(39.) a man steps backwards slowly.\n",
      "30 tensor(15.) a person bounced six times.\n",
      "31 tensor(27.) a person pitching a baseballl\n"
     ]
    }
   ],
   "source": [
    "__break__ = False\n",
    "while __break__ is False:\n",
    "    for word_embeddings, pos_one_hots, clip_text, sent_len, pose, m_length, token in eval_val_loader:\n",
    "        for i, text in enumerate(clip_text):\n",
    "            # if 'jump' in text:\n",
    "            __break__ = True\n",
    "            print(i, m_length[i]/4, text)\n",
    "                # break\n",
    "        if __break__:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.02, 0.06, 0.15, 0.43, 0.16, 0.14, 0.03, 0.  ,\n",
       "        0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  ]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " <a list of 49 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZp0lEQVR4nO3db2yW1f348U+htjWMdgpaIFSsbiqKstluWJTp/NOFMTPjgzFdQCNkdgFn1+0BSDKR/SlZHMNFAYl/NtzULlGXJbKMJlPEMRPpSiRqFjOBEix2sKzt2CyzXN8H/mx+XUF6l7bHltcruRLuc5/r7ukJ0Heu3r2al2VZFgAAiYxJvQAA4NQmRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAIKn81Avoj6NHj8Y777wT48ePj7y8vNTLAQD6Icuy6OzsjClTpsSYMce//jEiYuSdd96JsrKy1MsAAAZg3759MXXq1OM+PyJiZPz48RHxwSdTXFyceDUAQH90dHREWVlZz9fx4xkRMfLht2aKi4vFCACMMCd6i4U3sAIASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkspPvQA41Zy77PkTztmzet4wrATg48GVEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkNSAYmTdunVRXl4eRUVFUVFREdu2bevXeX/6058iPz8/PvOZzwzkwwIAo1DOMdLQ0BC1tbWxYsWKaG5ujjlz5sTcuXOjpaXlI89rb2+PhQsXxnXXXTfgxQIAo0/OMbJmzZpYtGhRLF68OKZPnx5r166NsrKyWL9+/Ueed+edd8att94aVVVVA14sADD65BQjR44ciaampqiuru41Xl1dHdu3bz/ueY8//nj87W9/i3vvvbdfH6erqys6Ojp6HQDA6JRTjBw8eDC6u7ujtLS013hpaWkcOHDgmOe89dZbsWzZsvj1r38d+fn5/fo49fX1UVJS0nOUlZXlskwAYAQZ0BtY8/Lyej3OsqzPWEREd3d33HrrrXHffffFBRdc0O/XX758ebS3t/cc+/btG8gyAYARoH+XKv6fiRMnxtixY/tcBWlra+tztSQiorOzM3bs2BHNzc2xdOnSiIg4evRoZFkW+fn5sWXLlrj22mv7nFdYWBiFhYW5LA0AGKFyujJSUFAQFRUV0djY2Gu8sbExZs+e3Wd+cXFx7Nq1K3bu3Nlz1NTUxIUXXhg7d+6MWbNmndzqAYARL6crIxERdXV1sWDBgqisrIyqqqrYuHFjtLS0RE1NTUR88C2W/fv3x6ZNm2LMmDExY8aMXuefffbZUVRU1GccADg15Rwj8+fPj0OHDsWqVauitbU1ZsyYEZs3b45p06ZFRERra+sJ7zkCAPChvCzLstSLOJGOjo4oKSmJ9vb2KC4uTr0cOCnnLnv+hHP2rJ43DCsBGFr9/frtd9MAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFIDipF169ZFeXl5FBUVRUVFRWzbtu24c19++eW48sorY8KECXH66afHRRddFD/72c8GvGAAYHTJz/WEhoaGqK2tjXXr1sWVV14ZDz/8cMydOzfeeOONOOecc/rMHzduXCxdujQuu+yyGDduXLz88stx5513xrhx4+Kb3/zmoHwSAMDIlZdlWZbLCbNmzYrLL7881q9f3zM2ffr0uOmmm6K+vr5fr3HzzTfHuHHj4oknnujX/I6OjigpKYn29vYoLi7OZbnwsXPusudPOGfP6nnDsBKAodXfr985fZvmyJEj0dTUFNXV1b3Gq6urY/v27f16jebm5ti+fXtcffXVx53T1dUVHR0dvQ4AYHTKKUYOHjwY3d3dUVpa2mu8tLQ0Dhw48JHnTp06NQoLC6OysjKWLFkSixcvPu7c+vr6KCkp6TnKyspyWSYAMIIM6A2seXl5vR5nWdZn7H9t27YtduzYERs2bIi1a9fGU089ddy5y5cvj/b29p5j3759A1kmADAC5PQG1okTJ8bYsWP7XAVpa2vrc7Xkf5WXl0dExKWXXhrvvvturFy5Mm655ZZjzi0sLIzCwsJclgYAjFA5XRkpKCiIioqKaGxs7DXe2NgYs2fP7vfrZFkWXV1duXxoAGCUyvlHe+vq6mLBggVRWVkZVVVVsXHjxmhpaYmampqI+OBbLPv3749NmzZFRMRDDz0U55xzTlx00UUR8cF9R+6///646667BvHTAABGqpxjZP78+XHo0KFYtWpVtLa2xowZM2Lz5s0xbdq0iIhobW2NlpaWnvlHjx6N5cuXx+7duyM/Pz/OP//8WL16ddx5552D91kAACNWzvcZScF9RhhN3GcEOFUMyX1GAAAGmxgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAIKkBxci6deuivLw8ioqKoqKiIrZt23bcuc8++2zccMMNcdZZZ0VxcXFUVVXFH/7whwEvGAAYXXKOkYaGhqitrY0VK1ZEc3NzzJkzJ+bOnRstLS3HnP/SSy/FDTfcEJs3b46mpqb44he/GDfeeGM0Nzef9OIBgJEvL8uyLJcTZs2aFZdffnmsX7++Z2z69Olx0003RX19fb9e45JLLon58+fH97///X7N7+joiJKSkmhvb4/i4uJclgsfO+cue/6Ec/asnjcMKwEYWv39+p3TlZEjR45EU1NTVFdX9xqvrq6O7du39+s1jh49Gp2dnXHmmWfm8qEBgFEqP5fJBw8ejO7u7igtLe01XlpaGgcOHOjXa/z0pz+Nw4cPx9e+9rXjzunq6oqurq6exx0dHbksEwAYQQb0Bta8vLxej7Ms6zN2LE899VSsXLkyGhoa4uyzzz7uvPr6+igpKek5ysrKBrJMAGAEyClGJk6cGGPHju1zFaStra3P1ZL/1dDQEIsWLYrf/OY3cf3113/k3OXLl0d7e3vPsW/fvlyWCQCMIDnFSEFBQVRUVERjY2Ov8cbGxpg9e/Zxz3vqqafi9ttvjyeffDLmzTvxG/MKCwujuLi41wEAjE45vWckIqKuri4WLFgQlZWVUVVVFRs3boyWlpaoqamJiA+uauzfvz82bdoUER+EyMKFC+OBBx6IK664oueqyumnnx4lJSWD+KkAACNRzjEyf/78OHToUKxatSpaW1tjxowZsXnz5pg2bVpERLS2tva658jDDz8c77//fixZsiSWLFnSM37bbbfFL37xi5P/DACAES3n+4yk4D4jjCbuMwKcKobkPiMAAINNjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASeWnXgDQ17nLnj/hnD2r5w3DSgCGnisjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlPuMwCDqz/1BAOjNlREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASbnpGYxQ/bnB2p7V84ZhJQAnx5URACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkBhQj69ati/Ly8igqKoqKiorYtm3bcee2trbGrbfeGhdeeGGMGTMmamtrB7xYAGD0yTlGGhoaora2NlasWBHNzc0xZ86cmDt3brS0tBxzfldXV5x11lmxYsWKmDlz5kkvGAAYXXKOkTVr1sSiRYti8eLFMX369Fi7dm2UlZXF+vXrjzn/3HPPjQceeCAWLlwYJSUlJ71gAGB0ySlGjhw5Ek1NTVFdXd1rvLq6OrZv3z5oi+rq6oqOjo5eBwAwOuUUIwcPHozu7u4oLS3tNV5aWhoHDhwYtEXV19dHSUlJz1FWVjZorw0AfLwM6A2seXl5vR5nWdZn7GQsX7482tvbe459+/YN2msDAB8v+blMnjhxYowdO7bPVZC2trY+V0tORmFhYRQWFg7a6wEAH185XRkpKCiIioqKaGxs7DXe2NgYs2fPHtSFAQCnhpyujERE1NXVxYIFC6KysjKqqqpi48aN0dLSEjU1NRHxwbdY9u/fH5s2beo5Z+fOnRER8a9//Sv+/ve/x86dO6OgoCAuvvjiQfo0AICRKucYmT9/fhw6dChWrVoVra2tMWPGjNi8eXNMmzYtIj64ydn/3nPks5/9bM+fm5qa4sknn4xp06bFnj17Tm71AMCIl5dlWZZ6ESfS0dERJSUl0d7eHsXFxamXA8d17rLnUy+hlz2r56VeAnAK6+/Xb7+bBgBISowAAEmJEQAgKTECACQlRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEnlp14AjATnLns+9RIARi1XRgCApMQIAJCUGAEAkhIjAEBSYgQASEqMAABJiREAICkxAgAkJUYAgKTECACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEmJEQAgKTECACSVn3oBwNA5d9nz/Zq3Z/W8IV4JwPG5MgIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEm5zwinvP7eiwOAoeHKCACQlBgBAJISIwBAUmIEAEhKjAAASYkRACApMQIAJOU+I8Cg6c89W/asnjcMKwFGEldGAICkxAgAkJQYAQCSGlCMrFu3LsrLy6OoqCgqKipi27ZtHzl/69atUVFREUVFRXHeeefFhg0bBrRYAGD0yTlGGhoaora2NlasWBHNzc0xZ86cmDt3brS0tBxz/u7du+PLX/5yzJkzJ5qbm+Oee+6Jb3/72/HMM8+c9OIBgJEv5xhZs2ZNLFq0KBYvXhzTp0+PtWvXRllZWaxfv/6Y8zds2BDnnHNOrF27NqZPnx6LFy+OO+64I+6///6TXjwAMPLl9KO9R44ciaampli2bFmv8erq6ti+ffsxz/nzn/8c1dXVvca+9KUvxaOPPhr//e9/47TTTutzTldXV3R1dfU8bm9vj4iIjo6OXJYL/XK069+pl5DcYP3b6s9e+ncMp44P/71nWfaR83KKkYMHD0Z3d3eUlpb2Gi8tLY0DBw4c85wDBw4cc/77778fBw8ejMmTJ/c5p76+Pu67774+42VlZbksF+inkrWj82MBHw+dnZ1RUlJy3OcHdNOzvLy8Xo+zLOszdqL5xxr/0PLly6Ourq7n8dGjR+Mf//hHTJgw4SM/Tq46OjqirKws9u3bF8XFxYP2uhyb/R5e9nt42e/hZb+H30D2PMuy6OzsjClTpnzkvJxiZOLEiTF27Ng+V0Ha2tr6XP340KRJk445Pz8/PyZMmHDMcwoLC6OwsLDX2Cc/+clclpqT4uJif5mHkf0eXvZ7eNnv4WW/h1+ue/5RV0Q+lNMbWAsKCqKioiIaGxt7jTc2Nsbs2bOPeU5VVVWf+Vu2bInKyspjvl8EADi15PzTNHV1dfHII4/EY489Fm+++WZ85zvfiZaWlqipqYmID77FsnDhwp75NTU1sXfv3qirq4s333wzHnvssXj00Ufje9/73uB9FgDAiDV25cqVK3M5YcaMGTFhwoT48Y9/HPfff3/85z//iSeeeCJmzpwZERG/+tWvYu/evXH77bdHRMQZZ5wRV111VTz88MPxgx/8IJqbm+NHP/pRr2BJaezYsXHNNddEfr7fGTgc7Pfwst/Dy34PL/s9/IZqz/OyE/28DQDAEPK7aQCApMQIAJCUGAEAkhIjAEBSp3SMrFu3LsrLy6OoqCgqKipi27ZtqZc0Krz00ktx4403xpQpUyIvLy9++9vf9no+y7JYuXJlTJkyJU4//fS45ppr4vXXX0+02pGvvr4+Pve5z8X48ePj7LPPjptuuin++te/9ppjzwfP+vXr47LLLuu58VNVVVX8/ve/73neXg+d+vr6yMvLi9ra2p4x+z24Vq5cGXl5eb2OSZMm9Tw/VPt9ysZIQ0ND1NbWxooVK6K5uTnmzJkTc+fOjZaWltRLG/EOHz4cM2fOjAcffPCYz//kJz+JNWvWxIMPPhivvvpqTJo0KW644Ybo7Owc5pWODlu3bo0lS5bEK6+8Eo2NjfH+++9HdXV1HD58uGeOPR88U6dOjdWrV8eOHTtix44dce2118ZXv/rVnv+Q7fXQePXVV2Pjxo1x2WWX9Rq334PvkksuidbW1p5j165dPc8N2X5np6jPf/7zWU1NTa+xiy66KFu2bFmiFY1OEZE999xzPY+PHj2aTZo0KVu9enXP2HvvvZeVlJRkGzZsSLHEUaetrS2LiGzr1q1Zltnz4XDGGWdkjzzyiL0eIp2dndmnP/3prLGxMbv66quzu+++O8syf7eHwr333pvNnDnzmM8N5X6fkldGjhw5Ek1NTVFdXd1rvLq6OrZv355oVaeG3bt3x4EDB3rtfWFhYVx99dX2fpC0t7dHRMSZZ54ZEfZ8KHV3d8fTTz8dhw8fjqqqKns9RJYsWRLz5s2L66+/vte4/R4ab731VkyZMiXKy8vj61//erz99tsRMbT7fUretu7gwYPR3d3d55f7lZaW9vmlfgyuD/f3WHu/d+/eFEsaVbIsi7q6urjqqqtixowZEWHPh8KuXbuiqqoq3nvvvfjEJz4Rzz33XFx88cU9/yHb68Hz9NNPx1/+8pd49dVX+zzn7/bgmzVrVmzatCkuuOCCePfdd+OHP/xhzJ49O15//fUh3e9TMkY+lJeX1+txlmV9xhga9n5oLF26NF577bV4+eWX+zxnzwfPhRdeGDt37ox//vOf8cwzz8Rtt90WW7du7XneXg+Offv2xd133x1btmyJoqKi486z34Nn7ty5PX++9NJLo6qqKs4///z45S9/GVdccUVEDM1+n5Lfppk4cWKMHTu2z1WQtra2PsXH4PrwXdn2fvDddddd8bvf/S5eeOGFmDp1as+4PR98BQUF8alPfSoqKyujvr4+Zs6cGQ888IC9HmRNTU3R1tYWFRUVkZ+fH/n5+bF169b4+c9/Hvn5+T17ar+Hzrhx4+LSSy+Nt956a0j/fp+SMVJQUBAVFRXR2NjYa7yxsTFmz56daFWnhvLy8pg0aVKvvT9y5Ehs3brV3g9QlmWxdOnSePbZZ+OPf/xjlJeX93reng+9LMuiq6vLXg+y6667Lnbt2hU7d+7sOSorK+Mb3/hG7Ny5M8477zz7PcS6urrizTffjMmTJw/t3++TevvrCPb0009np512Wvboo49mb7zxRlZbW5uNGzcu27NnT+qljXidnZ1Zc3Nz1tzcnEVEtmbNmqy5uTnbu3dvlmVZtnr16qykpCR79tlns127dmW33HJLNnny5KyjoyPxykemb33rW1lJSUn24osvZq2trT3Hv//975459nzwLF++PHvppZey3bt3Z6+99lp2zz33ZGPGjMm2bNmSZZm9Hmr//0/TZJn9Hmzf/e53sxdffDF7++23s1deeSX7yle+ko0fP77na+NQ7fcpGyNZlmUPPfRQNm3atKygoCC7/PLLe34UkpPzwgsvZBHR57jtttuyLPvgx8PuvffebNKkSVlhYWH2hS98Idu1a1faRY9gx9rriMgef/zxnjn2fPDccccdPf9vnHXWWdl1113XEyJZZq+H2v/GiP0eXPPnz88mT56cnXbaadmUKVOym2++OXv99dd7nh+q/c7Lsiw7uWsrAAADd0q+ZwQA+PgQIwBAUmIEAEhKjAAASYkRACApMQIAJCVGAICkxAgAkJQYAQCSEiMAQFJiBABISowAAEn9H8sW2dMKFMJbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 9\n",
    "mids = t2m_transformer.generate([clip_text[i]]*100, m_length.cuda(), timesteps = 3, cond_scale = 4, temperature=1) # , temp=code_idx[..., 0]\n",
    "mids, pred_len = t2m_transformer.pad_when_end(mids)\n",
    "# plt.bar(bins[:-1], counts/1e3, label=\"Bars 1\") # , color=\"blue\"\n",
    "density, bins = np.histogram(pred_len.detach().cpu().numpy(), bins=np.arange(50))\n",
    "unity_density = density / density.sum()\n",
    "plt.hist(bins[:-1], bins, weights=unity_density)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
